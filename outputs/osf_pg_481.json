{"data": [{"id": "gner5", "type": "registrations", "attributes": {"title": "EmilyHeffernan_MastersProject", "description": "Pre-registration for master's project on categorical learning mechanism in the hippocampus, combining experimental data, DTI, and a neural network model.", "category": "project", "custom_citation": "", "date_created": "2020-01-30T14:29:27.480976", "date_modified": "2019-09-06T17:15:17.149121", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": null, "analytics_key": "a60d5c1d84cca0999280383e8d33d107d1a17ff50b44bcf4631d063d0b42cdb047c14981463488dd6b4d3eb1f50c428584548e7462499eea2556888943d69d0b941e8179476f955f3c3cac37e32289aa8aaeae18750a3fd142da9d12469b0f910ac687ca5f7babf0e45bc018a689857211a2436d8bdca3289061bf5d632f5a548e2d6d0b7e4584bab085e33ff2103746", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-30T14:29:27.465255", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "Preregistration Template from AsPredicted.org", "registered_meta": {"data": {"extra": [], "value": "It's complicated. We have already collected some data but explain in Question 8 why readers may consider this a valid pre-registration nevertheless."}, "name": {"extra": [], "value": "Manipulating the order of stimulus presentation to modulate category learning performance: A behavioural pilot study"}, "other": {"extra": [], "value": "Although data collection for this project began on Jan 23, 2020, the data will not be examined until this pre-registration has been submitted.\n\nFollowing the categorization task, the subjects will be asked to transcribe any rule they developed or used when categorizing stimuli. The complexity and accuracy of these rules will be assessed. \n\nParticipants will also complete a \u201cnew vs. old\u201d task in which they identify whether stimuli are new or old. D\u2019 and criterion statistics will be calculated to determine whether participants exhibit enhanced recognition of exception items.\n\nA neural network model of the hippocampus created by Schapiro et al. (2017) will be used to further analyze the data. The model is able to stimulate lesions to the TSP and MSP, and we will assess whether adjusting these weights to reflect in vivo TSP/MSP integrity of participants will replicate the categorization performance of participants. "}, "sample": {"extra": [], "value": "Data from 40 subjects who demonstrate sufficient learning (as defined in question 6) will be collected. This number is based on results from a previous study that used a similar task and will ensure sufficient power."}, "analyses": {"extra": [], "value": "A logistic regression model will be used to assess whether the effects of exception salience impact participants\u2019 ability to learn categories/exception items. The same model will be applied to the log transform of the reaction times. "}, "outliers": {"extra": [], "value": "Participants will be screened for any visual impairments. Participants with normal (or corrected-to-normal) vision will be accepted. Participants will also be excluded if they do not speak English.\n\nParticipants who fail to achieve an accuracy of 75% for prototype items in at least one of the learning blocks will be excluded. Moreover, trials with a reaction time of less than 150 ms or more than 2000 ms will be excluded. \n"}, "dependent": {"extra": [], "value": "Stimuli will be presented in blocks in order to measure learning over time. In the learning task, three dependent variables will be of interest: the probability of error across trials when categorizing (i) prototype stimuli, (ii) similar stimuli, and (iii) exception stimuli. "}, "conditions": {"extra": [], "value": "Participants will be assigned to one of two conditions. In condition one, the \u201cearly exceptions\u201d condition, participants will be exposed to exception stimuli in the first learning block; in condition two, they will not be exposed to exception stimuli until the second learning block. Assignment into each condition will be based on sequential participant number (i.e., participant one, three, five, et cetera will be assigned to condition one; participants two, four, and so on will be assigned to condition two)."}, "hypothesis": {"extra": [], "value": "Participants will complete a categorical learning task in which they must sort stimuli into two separate categories. The stimuli, various flowers, will vary along three dimensions, namely pistil colour, petal colour, number of outer petals, and number of inner petals; only the latter three will be relevant to making correct categorical decisions. The classification problem will be a \u201cType iii\u201d problem, as defined by Shepard et al. (1961). This type of problem is of intermediate complexity; the participant must consider all three dimensions, but not all three dimensions are of equal importance. The structure of this problem is also known as \u201csingle dimension plus exception\u201d. Participants will learn to categorize the flowers through trial and error by completing three learning blocks with feedback. Following the three learning blocks, participants will complete a categorization block without feedback. Finally, they will complete a surprise recognition task in which they will categorize flower images as \u201cnew\u201d or \u201cold\u201d. There are three types of stimuli in this task: prototype items (the prototypes for each category differ across all three dimensions); similar items, which differ from their category prototype across one dimension; and exception items, which differ from their category prototype across two dimensions. We predict that participants will exhibit superior classification of prototype items, followed by similar items. Classification of exception items will be poor but should be modulated by the conditions described in question three. A future study will relate this altered performance to neuroimaging data. Participants should also exhibit improved recognition of exception items. "}, "study_type": {"extra": [], "value": "Other (describe below)"}, "study_type_other": {"extra": [], "value": "Research project for master's thesis"}}, "registration_responses": {"data": "It's complicated. We have already collected some data but explain in Question 8 why readers may consider this a valid pre-registration nevertheless.", "name": "Manipulating the order of stimulus presentation to modulate category learning performance: A behavioural pilot study", "other": "Although data collection for this project began on Jan 23, 2020, the data will not be examined until this pre-registration has been submitted.\n\nFollowing the categorization task, the subjects will be asked to transcribe any rule they developed or used when categorizing stimuli. The complexity and accuracy of these rules will be assessed. \n\nParticipants will also complete a \u201cnew vs. old\u201d task in which they identify whether stimuli are new or old. D\u2019 and criterion statistics will be calculated to determine whether participants exhibit enhanced recognition of exception items.\n\nA neural network model of the hippocampus created by Schapiro et al. (2017) will be used to further analyze the data. The model is able to stimulate lesions to the TSP and MSP, and we will assess whether adjusting these weights to reflect in vivo TSP/MSP integrity of participants will replicate the categorization performance of participants. ", "sample": "Data from 40 subjects who demonstrate sufficient learning (as defined in question 6) will be collected. This number is based on results from a previous study that used a similar task and will ensure sufficient power.", "analyses": "A logistic regression model will be used to assess whether the effects of exception salience impact participants\u2019 ability to learn categories/exception items. The same model will be applied to the log transform of the reaction times. ", "outliers": "Participants will be screened for any visual impairments. Participants with normal (or corrected-to-normal) vision will be accepted. Participants will also be excluded if they do not speak English.\n\nParticipants who fail to achieve an accuracy of 75% for prototype items in at least one of the learning blocks will be excluded. Moreover, trials with a reaction time of less than 150 ms or more than 2000 ms will be excluded. \n", "dependent": "Stimuli will be presented in blocks in order to measure learning over time. In the learning task, three dependent variables will be of interest: the probability of error across trials when categorizing (i) prototype stimuli, (ii) similar stimuli, and (iii) exception stimuli. ", "conditions": "Participants will be assigned to one of two conditions. In condition one, the \u201cearly exceptions\u201d condition, participants will be exposed to exception stimuli in the first learning block; in condition two, they will not be exposed to exception stimuli until the second learning block. Assignment into each condition will be based on sequential participant number (i.e., participant one, three, five, et cetera will be assigned to condition one; participants two, four, and so on will be assigned to condition two).", "hypothesis": "Participants will complete a categorical learning task in which they must sort stimuli into two separate categories. The stimuli, various flowers, will vary along three dimensions, namely pistil colour, petal colour, number of outer petals, and number of inner petals; only the latter three will be relevant to making correct categorical decisions. The classification problem will be a \u201cType iii\u201d problem, as defined by Shepard et al. (1961). This type of problem is of intermediate complexity; the participant must consider all three dimensions, but not all three dimensions are of equal importance. The structure of this problem is also known as \u201csingle dimension plus exception\u201d. Participants will learn to categorize the flowers through trial and error by completing three learning blocks with feedback. Following the three learning blocks, participants will complete a categorization block without feedback. Finally, they will complete a surprise recognition task in which they will categorize flower images as \u201cnew\u201d or \u201cold\u201d. There are three types of stimuli in this task: prototype items (the prototypes for each category differ across all three dimensions); similar items, which differ from their category prototype across one dimension; and exception items, which differ from their category prototype across two dimensions. We predict that participants will exhibit superior classification of prototype items, followed by similar items. Classification of exception items will be poor but should be modulated by the conditions described in question three. A future study will relate this altered performance to neuroimaging data. Participants should also exhibit improved recognition of exception items. ", "study_type": "Other (describe below)", "study_type_other": "Research project for master's thesis"}, "subjects": []}, "relationships": {"children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/comments/?format=json&filter%5Btarget%5D=gner5", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/gner5/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/ca-1/?format=json", "meta": {}}}, "data": {"id": "ca-1", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/?format=json", "meta": {}}}, "data": {"id": "gner5", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/gner5/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/gner5/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gner5/citation/?format=json", "meta": {}}}, "data": {"id": "gner5", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/t5z9f/?format=json", "meta": {}}}, "data": {"id": "t5z9f", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/y2g3s/?format=json", "meta": {}}}, "data": {"id": "y2g3s", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5d2d2268d28338002c2432d2/?format=json", "meta": {}}}, "data": {"id": "5d2d2268d28338002c2432d2", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/gner5/", "self": "https://api.osf.io/v2/registrations/gner5/"}}, {"id": "gr72s", "type": "registrations", "attributes": {"title": "Data from: Dynamical determinants of different spine movements and gait speeds in rotary and transverse gallops", "description": "Data from: \nDynamical determinants of different spine movements and gait speeds in rotary and transverse gallops", "category": "data", "custom_citation": "", "date_created": "2020-01-30T09:16:37.164252", "date_modified": "2020-01-16T08:48:37.520928", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": {"copyright_holders": [""], "year": "2019"}, "analytics_key": "ba1be2e6805fe799ca846e42ef18953dc474c86606354308697195d75f0d5bcce7cff6274ad12797c1718fe3aa20ad04bb7aa4beafdd7a258373a636a165e9184e1817f65dc5d083a847a0b8920cba42d76d11a6213153a4a4f90bf33dcecb5b10d7f7237dea7156b5178e018fd857ff27de710977b55f03738fbbe5d9c1ef70cbfed6e26c1606a9e18169ce69da82aa", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-30T09:16:37.125316", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "OSF Preregistration", "registered_meta": {"q1": {"extra": [], "value": "Data from: Dynamical determinants of different spine movements and gait speeds in rotary and transverse gallop"}, "q2": {"extra": [], "value": "Tomoya Kamimura"}, "q3": {"extra": [], "value": ""}, "q4": {"extra": [], "value": "Our primary scientific questions do not involve binary hypothesis testing."}, "q5": {"extra": [], "value": "Observational Study - Data is collected from study subjects that are not randomly assigned to a treatment. This includes surveys, \u201cnatural experiments,\u201d and regression discontinuity designs."}, "q6": {"extra": [], "value": ["No blinding is involved in this study."]}, "q7": {"extra": [], "value": ""}, "q8": {"value": {"question": {"extra": [], "value": "This study is based on a simple model. The data were collected to verify the validity of our model. All subjects were required to perform galloping gait."}, "uploader": {"extra": [{"data": {"name": "Data_for_estimating_h1_and_h2.xlsx"}, "nodeId": "m7dty", "sha256": "2d679f9b99c9f53fc20452eb412c65077efc6372e98b4edf107df46f8cb1ea5a", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1b/", "selectedFileName": "Data_for_estimating_h1_and_h2.xlsx"}, {"data": {"name": "Data_for_estimating_c1_and_c2.xlsx"}, "nodeId": "m7dty", "sha256": "2b160269a11471d79939ed3a3195e923c9c77aebf5be8b0d3cb0a68432c41436", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c19/", "selectedFileName": "Data_for_estimating_c1_and_c2.xlsx"}, {"data": {"name": "Data_for_estimating_H_and_D.xlsx"}, "nodeId": "m7dty", "sha256": "a22b916c702427a3f45ed13f4f6ed26692ed5a8b3ca50348f78401f2a8a412fc", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c15/", "selectedFileName": "Data_for_estimating_H_and_D.xlsx"}], "value": ""}}}, "q9": {"extra": [], "value": ""}, "q10": {"extra": [], "value": "Registration following analysis of the data"}, "q11": {"extra": [], "value": ""}, "q12": {"value": {"question": {"extra": [], "value": "Data were measured from experiments and existing pictures. All corrected data were used and there is no exclusion."}, "uploader": {"extra": [{"data": {"name": "Data_for_estimating_M_L_J.xlsx"}, "nodeId": "m7dty", "sha256": "c5d0e7da5bd3dfee3b2f7fa1682d556bc60a4636294f42b249f65e2002dce663", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1d/", "selectedFileName": "Data_for_estimating_M_L_J.xlsx"}, {"data": {"name": "Data_for_estimating_h1_and_h2.xlsx"}, "nodeId": "m7dty", "sha256": "2d679f9b99c9f53fc20452eb412c65077efc6372e98b4edf107df46f8cb1ea5a", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1b/", "selectedFileName": "Data_for_estimating_h1_and_h2.xlsx"}, {"data": {"name": "Data_for_estimating_c1_and_c2.xlsx"}, "nodeId": "m7dty", "sha256": "2b160269a11471d79939ed3a3195e923c9c77aebf5be8b0d3cb0a68432c41436", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c19/", "selectedFileName": "Data_for_estimating_c1_and_c2.xlsx"}, {"data": {"name": "Data_for_estimating_H_and_D.xlsx"}, "nodeId": "m7dty", "sha256": "a22b916c702427a3f45ed13f4f6ed26692ed5a8b3ca50348f78401f2a8a412fc", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c15/", "selectedFileName": "Data_for_estimating_H_and_D.xlsx"}, {"data": {"name": "Data_estimated_parameters.xlsx"}, "nodeId": "m7dty", "sha256": "45f262c8fae44dd754b51a7e1e795874a26fb133466a5ed9540155354f723626", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c17/", "selectedFileName": "Data_estimated_parameters.xlsx"}], "value": ""}}}, "q13": {"extra": [], "value": "One cheetah CT data were used for body parameter estimation. Four cheetahs and a horse performed experiments to estimate the kinematic data. "}, "q14": {"extra": [], "value": ""}, "q15": {"extra": [], "value": ""}, "q16": {"value": {"question": {"extra": [], "value": ""}, "uploader": {"extra": [], "value": ""}}}, "q17": {"value": {"question": {"extra": [], "value": "Body parameters and kinematic data in galloping"}, "uploader": {"extra": [{"data": {"name": "Data_for_estimating_M_L_J.xlsx"}, "nodeId": "m7dty", "sha256": "c5d0e7da5bd3dfee3b2f7fa1682d556bc60a4636294f42b249f65e2002dce663", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1d/", "selectedFileName": "Data_for_estimating_M_L_J.xlsx"}, {"data": {"name": "Data_for_estimating_h1_and_h2.xlsx"}, "nodeId": "m7dty", "sha256": "2d679f9b99c9f53fc20452eb412c65077efc6372e98b4edf107df46f8cb1ea5a", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1b/", "selectedFileName": "Data_for_estimating_h1_and_h2.xlsx"}, {"data": {"name": "Data_for_estimating_c1_and_c2.xlsx"}, "nodeId": "m7dty", "sha256": "2b160269a11471d79939ed3a3195e923c9c77aebf5be8b0d3cb0a68432c41436", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c19/", "selectedFileName": "Data_for_estimating_c1_and_c2.xlsx"}, {"data": {"name": "Data_estimated_parameters.xlsx"}, "nodeId": "m7dty", "sha256": "45f262c8fae44dd754b51a7e1e795874a26fb133466a5ed9540155354f723626", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c17/", "selectedFileName": "Data_estimated_parameters.xlsx"}, {"data": {"name": "Data_for_estimating_H_and_D.xlsx"}, "nodeId": "m7dty", "sha256": "a22b916c702427a3f45ed13f4f6ed26692ed5a8b3ca50348f78401f2a8a412fc", "viewUrl": "/project/gr72s/files/osfstorage/5e329ef84b2010005f212c15/", "selectedFileName": "Data_for_estimating_H_and_D.xlsx"}], "value": ""}}}, "q18": {"value": {"question": {"extra": [], "value": ""}, "uploader": {"extra": [], "value": ""}}}, "q19": {"value": {"question": {"extra": [], "value": "There are no binary hypothesis in our study."}, "uploader": {"extra": [], "value": ""}}}, "q20": {"extra": [], "value": ""}, "q21": {"extra": [], "value": ""}, "q22": {"extra": [], "value": ""}, "q23": {"extra": [], "value": ""}, "q24": {"extra": [], "value": ""}, "q25": {"extra": [], "value": ""}}, "registration_responses": {"q1": "Data from: Dynamical determinants of different spine movements and gait speeds in rotary and transverse gallop", "q2": "Tomoya Kamimura", "q3": "", "q4": "Our primary scientific questions do not involve binary hypothesis testing.", "q5": "Observational Study - Data is collected from study subjects that are not randomly assigned to a treatment. This includes surveys, \u201cnatural experiments,\u201d and regression discontinuity designs.", "q6": ["No blinding is involved in this study."], "q7": "", "q9": "", "q10": "Registration following analysis of the data", "q11": "", "q13": "One cheetah CT data were used for body parameter estimation. Four cheetahs and a horse performed experiments to estimate the kinematic data. ", "q14": "", "q15": "", "q20": "", "q21": "", "q22": "", "q23": "", "q24": "", "q25": "", "q8.question": "This study is based on a simple model. The data were collected to verify the validity of our model. All subjects were required to perform galloping gait.", "q8.uploader": [{"file_id": "5e329ef84b2010005f212c1b", "file_name": "Data_for_estimating_h1_and_h2.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1b", "download": "https://osf.io/download/5e329ef84b2010005f212c1b"}, "file_hashes": {"sha256": "2d679f9b99c9f53fc20452eb412c65077efc6372e98b4edf107df46f8cb1ea5a"}}, {"file_id": "5e329ef84b2010005f212c19", "file_name": "Data_for_estimating_c1_and_c2.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c19", "download": "https://osf.io/download/5e329ef84b2010005f212c19"}, "file_hashes": {"sha256": "2b160269a11471d79939ed3a3195e923c9c77aebf5be8b0d3cb0a68432c41436"}}, {"file_id": "5e329ef84b2010005f212c15", "file_name": "Data_for_estimating_H_and_D.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c15", "download": "https://osf.io/download/5e329ef84b2010005f212c15"}, "file_hashes": {"sha256": "a22b916c702427a3f45ed13f4f6ed26692ed5a8b3ca50348f78401f2a8a412fc"}}], "q12.question": "Data were measured from experiments and existing pictures. All corrected data were used and there is no exclusion.", "q12.uploader": [{"file_id": "5e329ef84b2010005f212c1d", "file_name": "Data_for_estimating_M_L_J.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1d", "download": "https://osf.io/download/5e329ef84b2010005f212c1d"}, "file_hashes": {"sha256": "c5d0e7da5bd3dfee3b2f7fa1682d556bc60a4636294f42b249f65e2002dce663"}}, {"file_id": "5e329ef84b2010005f212c1b", "file_name": "Data_for_estimating_h1_and_h2.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1b", "download": "https://osf.io/download/5e329ef84b2010005f212c1b"}, "file_hashes": {"sha256": "2d679f9b99c9f53fc20452eb412c65077efc6372e98b4edf107df46f8cb1ea5a"}}, {"file_id": "5e329ef84b2010005f212c19", "file_name": "Data_for_estimating_c1_and_c2.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c19", "download": "https://osf.io/download/5e329ef84b2010005f212c19"}, "file_hashes": {"sha256": "2b160269a11471d79939ed3a3195e923c9c77aebf5be8b0d3cb0a68432c41436"}}, {"file_id": "5e329ef84b2010005f212c15", "file_name": "Data_for_estimating_H_and_D.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c15", "download": "https://osf.io/download/5e329ef84b2010005f212c15"}, "file_hashes": {"sha256": "a22b916c702427a3f45ed13f4f6ed26692ed5a8b3ca50348f78401f2a8a412fc"}}, {"file_id": "5e329ef84b2010005f212c17", "file_name": "Data_estimated_parameters.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c17", "download": "https://osf.io/download/5e329ef84b2010005f212c17"}, "file_hashes": {"sha256": "45f262c8fae44dd754b51a7e1e795874a26fb133466a5ed9540155354f723626"}}], "q16.question": "", "q16.uploader": [], "q17.question": "Body parameters and kinematic data in galloping", "q17.uploader": [{"file_id": "5e329ef84b2010005f212c1d", "file_name": "Data_for_estimating_M_L_J.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1d", "download": "https://osf.io/download/5e329ef84b2010005f212c1d"}, "file_hashes": {"sha256": "c5d0e7da5bd3dfee3b2f7fa1682d556bc60a4636294f42b249f65e2002dce663"}}, {"file_id": "5e329ef84b2010005f212c1b", "file_name": "Data_for_estimating_h1_and_h2.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c1b", "download": "https://osf.io/download/5e329ef84b2010005f212c1b"}, "file_hashes": {"sha256": "2d679f9b99c9f53fc20452eb412c65077efc6372e98b4edf107df46f8cb1ea5a"}}, {"file_id": "5e329ef84b2010005f212c19", "file_name": "Data_for_estimating_c1_and_c2.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c19", "download": "https://osf.io/download/5e329ef84b2010005f212c19"}, "file_hashes": {"sha256": "2b160269a11471d79939ed3a3195e923c9c77aebf5be8b0d3cb0a68432c41436"}}, {"file_id": "5e329ef84b2010005f212c17", "file_name": "Data_estimated_parameters.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c17", "download": "https://osf.io/download/5e329ef84b2010005f212c17"}, "file_hashes": {"sha256": "45f262c8fae44dd754b51a7e1e795874a26fb133466a5ed9540155354f723626"}}, {"file_id": "5e329ef84b2010005f212c15", "file_name": "Data_for_estimating_H_and_D.xlsx", "file_urls": {"html": "https://osf.io/project/gr72s/files/osfstorage/5e329ef84b2010005f212c15", "download": "https://osf.io/download/5e329ef84b2010005f212c15"}, "file_hashes": {"sha256": "a22b916c702427a3f45ed13f4f6ed26692ed5a8b3ca50348f78401f2a8a412fc"}}], "q18.question": "", "q18.uploader": [], "q19.question": "There are no binary hypothesis in our study.", "q19.uploader": []}, "subjects": []}, "relationships": {"license": {"links": {"related": {"href": "https://api.osf.io/v2/licenses/563c1cf88c5e4a3877f9e96a/?format=json", "meta": {}}}, "data": {"id": "563c1cf88c5e4a3877f9e96a", "type": "licenses"}}, "children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/comments/?format=json&filter%5Btarget%5D=gr72s", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/gr72s/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/us/?format=json", "meta": {}}}, "data": {"id": "us", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/?format=json", "meta": {}}}, "data": {"id": "gr72s", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/gr72s/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/gr72s/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/gr72s/citation/?format=json", "meta": {}}}, "data": {"id": "gr72s", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/scazn/?format=json", "meta": {}}}, "data": {"id": "scazn", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/m7dty/?format=json", "meta": {}}}, "data": {"id": "m7dty", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5c08457ed283380029cf73bf/?format=json", "meta": {}}}, "data": {"id": "5c08457ed283380029cf73bf", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/gr72s/", "self": "https://api.osf.io/v2/registrations/gr72s/"}}, {"id": "jm47b", "type": "registrations", "attributes": {"title": "Modelling Domain General Metacognition", "description": "Our project modelling domain-general metacognition in a preregistered report.", "category": "project", "custom_citation": "", "date_created": "2020-01-30T11:01:13.208713", "date_modified": "2020-01-29T08:18:07.353790", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": null, "analytics_key": "2f09370274b643d810d256c80aa862072f235d84812d8e226c916fb68110cae0f5a6f0326ea0f69fd4c46d6f433de43e8c83348b8b428782fd46ecbc73c1de28d272793095684647c89d25b1fe1306eb75f122555ba059489611f5d58d792dd44b9bac2736014b3997758e5e7c0737c7ca7d22647c51538c5cc6e00766ecb27982ab5ee13851e588d7df9b2253329dc7", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-30T11:01:13.185922", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "OSF Preregistration", "registered_meta": {"q1": {"extra": [], "value": "Modelling Domain General Metacognition: A Confirmatory Study "}, "q2": {"extra": [], "value": "Micah Allen, Camile Correa"}, "q3": {"extra": [], "value": "Metacognition is the ability to monitor, evaluate and control one\u2019s own behavior (Fernandez-Duque, Baird, &amp; Posner, 2000; Flavell, 1979; Fleming &amp; Frith, 2014). Metacognitive ability is typically assessed by comparing subjective ratings (i.e., confidence) to objective task accuracy. Specifically, metacognition can be modelled using signal-detection theory (SDT) (Fleming &amp; Lau, 2014). This approach can quantify metacognitive performance while controlling for first-order judgement criterion, overall confidence level, and task accuracy, all of which can confound more straightforward measures of metacognition such as the raw correlation between confidence and accuracy (AKA, \u2018phi\u2019). Modelling confidence reports using SDT yields quantitative measures of metacognitive \u2018bias\u2019 (average confidence level irrespective of task performance), \u2018sensitivity\u2019 (how well subjective ratings distinguish between correct and incorrect responses\u2019), and \u2018efficiency\u2019 (metacognitive sensitivity relative to overall task performance). SDT-based measures have been applied to quantify metacognitive performance in a variety of cognitive, perceptual, and memory-related domains. Here a central question is whether metacognition is a global phenomenon, enabled by a domain-general central resource or is instead reliant on stratified domain-specific modules. For instance, McCurdy et al (2013) and Mazancieux et al (2018) both found a positive correlation between metacognitive accuracy across meta-memory and meta-perceptual tasks. However, Baird and colleagues repeatedly failed to replicate this finding (Baird, Cieslak, Smallwood, Grafton, &amp; Schooler, 2014; Baird, Mrazek, Phillips, &amp; Schooler, 2014; Baird, Smallwood, Gorgolewski, &amp; Margulies, 2013), and another study by Fleming and colleagues reported that while lesions to the rostrolateral prefrontal cortex diminished perceptual metacognition, metamemory was unimpaired (Fleming, Ryu, Golfinos, &amp; Blackmon, 2014). One recent meta-analysis found a high correlation between metacognition for different perceptual domains (i.e., auditory, tactile, or visual discrimination), but only a weak positive correlation between memory and perception (Rouault, McWilliams, Allen, &amp; Fleming, 2018). Collectively, these conflicting results highlight the substantial uncertainty regarding the question of domain generality in metacognition. In particular, to date most studies have not been pre-registered and rely on small sample sizes that may not reliably estimate the effect size of across-task correlations. Given the numerous possible researcher degrees of freedom involved in measuring metacognition, a well-powered, pre-registered confirmatory study is called for. To investigate the domain-generality (or lack thereof) of metacognitive ability, we will compare metacognitive bias, sensitivity, and efficiency across 3 different domains: vision, memory and general knowledge in up to 300 healthy participants. Additionally, in exploratory analyses we will use structural equation modelling, multivariate regression, and other techniques to explore the covariance between domain generality in metacognition, self-beliefs, and mind-wandering. \n\nAll study data, task code, and analysis code will be made available at the time of paper publication on our OSF Project Page and lab Github. \n"}, "q4": {"extra": [], "value": "Our central hypothesis tests will evaluate whether the linear correlation between metacognitive bias, sensitivity, or efficiency between any of the three tasks is greater than zero.\nIf metacognition is fully domain-specific, we expect to retain the null hypothesis of no significant correlation between the three tasks on any measure. In this case, we would expect to observe a null-bayes factor of at least 3 for one or more cross-task comparisons. \nIf metacognition is partially domain-specific, we expect to reject the null hypothesis of no significant correlation between at least two tasks on any two measures.\nIf metacognition is fully domain-general, we expect to reject the null hypothesis of no significant correlation across all three tasks, on at least one or more measures. \nThese results will strengthen or weaken the hypothesis of a common, domain general source supporting metacognitive efficiency, and shed light on what aspects of metacognition (bias, sensitivity, or efficiency) are shared versus unique across domains. \n"}, "q5": {"extra": [], "value": "Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials."}, "q6": {"extra": [], "value": ["No blinding is involved in this study."]}, "q7": {"extra": [], "value": "No"}, "q8": {"value": {"question": {"extra": [], "value": "The design is within subject. Participants will perform all tasks."}, "uploader": {"extra": [], "value": ""}}}, "q9": {"extra": [], "value": "All subjects will perform 3 tasks. The order of the tasks will be counterbalanced across subjects, but everyone will perform the same conditions."}, "q10": {"extra": [], "value": "Registration prior to creation of data"}, "q11": {"extra": [], "value": "Registration prior to creation of data.\n"}, "q12": {"value": {"question": {"extra": [], "value": "Participants will be recruited through advertisements at Sona system (CobeLab) from Aarhus University and also in social media (e.g., Twitter, Facebook). Participants must be at least 18 years old, give written consent, be normal or corrected to normal vision and fluent in English. Participants will be paid 110 DKK per hour and the estimated maximum duration of the test session is 2h."}, "uploader": {"extra": [], "value": ""}}}, "q13": {"extra": [], "value": "Our target sample size is 300 participants. We will attempt to recruit up to 330 in total, accounting for up to 10% data loss due to failure to complete the task according to the instructions, or other technical issues."}, "q14": {"extra": [], "value": "The study will compare metacognitive performance via correlations across three different tasks in 300 subjects. According to Sch\u00f6nbrodt and Perugini (2013) a minimum of n = 150 sample is required to reliably estimate between-subject correlation coefficients. Thus far the largest prior study in this area utilized n = 181 (Mazancieux et al., 2018), and all other previous studies utilized between 30 and 60 subjects (Rouault et al., 2018). Our study thus represents a 66.7% increase in sample size over the single largest prior study, and between x4-9 times increase in sample size relative to the majority of previous investigations. A priori power analysis using G*Power shows that this sample size will provide 99.5% power to detect \u201cmedium\u201d or larger effect sizes (Cohen\u2019s \u03c1 &gt; 0.3, two-tailed exact correlation test), and a 73% power to detect \u201csmall\u201d or larger effects (Cohen\u2019s \u03c1 &gt; 0.1). Further, our study will deploy 160 more trials per metacognition condition than Mazancieux et al. (2018); as the variance of metacognitive parameter estimation is directly related to the number of trials, this will further improve our power relative to all previous studies. "}, "q15": {"extra": [], "value": "We will stop when we finish collecting data from 330 participants, or when no more participants are available to participate. \n"}, "q16": {"value": {"question": {"extra": [], "value": "We do not have manipulated variables. \n"}, "uploader": {"extra": [], "value": ""}}}, "q17": {"value": {"question": {"extra": [], "value": "Participants will provide scores for the following 3 tasks:\n\nConfirmatory Measures:\n\nMetacognition Tasks Overview: Participants provide 2 alternative forced-choice (2AFC) responses in three psychological domains (visual perception, memory and general knowledge). On each trial, participants discriminate between the left and right stimulus using the left and right arrow keys. Participants are instructed to respond as quickly and accurately as possible. All tasks will contain 200 trials each. After each trial participants rate their confidence in the preceding decision using a 7-point likert scale. \n\nIndividual Tasks:\n\nMeta-visual task: The visual perception task will be the same as in several previous studies of metacognition (Fleming et al., 2014; Mazancieux et al., 2018; Rouault, Dayan, &amp; Fleming, 2019), code available here: https://github.com/metacoglab/meta_dots. In this 2AFC task, participants view two clouds of dots and discriminate which of the two had more total dots within it. Across trials, the difference between the two dot totals is staircased via 2 down 1 up procedure, which converges at the limit on 71% accuracy. \nMeta-memory task: The memory task is identical to that used in several previous studies (Mazancieux et al., 2018; McCurdy et al., 2013). In this task, across four blocks of fifty trials, subjects view a list of 50 words and are given either 30, 60, or 90 seconds to memorize the words. Afterwards, the memorized words are presented individually together with a distractor word from a non-memorized list, and the participant must discriminate which of the two words was seen before in a 2AFC recognition memory design. The order of the memorized vs distractor lists is counterbalanced across subjects in a full latin square design, and the order of the list study time (which is used to induce sufficient variability in discrimination accuracy) is randomized. \nMeta-knowledge \u2018Trivia\u2019 Task: This is a novel task developed for this and other studies by the current authors. The task is similar to previous investigations of metacognition which measured confidence and accuracy for \u2018domain general\u2019 or semantic knowledge tasks, e.g. about city size or other geographical facts (Mazancieux et al., 2018; Sanders, Hangya, &amp; Kepecs, 2016). A key difference is that here, participants perform \u2018trivia\u2019 judgements in multiple domains of knowledge; here, for both nutritional food and geopolitical domains (i.e., economics). In the first case, participants view two stimuli depicting 100g of food on separate plates (e.g., a plate of jelly beans and a plate of sausages). Participants must then determine which of the two plates has more total calories and rate their confidence in this choice. In the geopolitical domain, participants view flags of two countries and determine which had a higher average gross domestic product (GDP) over the previous 10 years. Crucially, and unlike previous tasks in this domain, the participant\u2019s prior experience or familiarity with each domain is titrated using an adaptive 2 down 1 up psychophysical procedure. In the case of foods, this involves adjusting the difference in calories between both food stimuli, and in the case of countries, by staircasing the log GDP difference. All food stimuli and associated caloric values are drawn from the Full4Health Image Collection (Charbonnier, van Meer, van der Laan, Viergever, &amp; Smeets, 2016). All GDP data where drawn from the World Bank Open Data library. \n\nExploratory Measures:\nSelf-belief: Participants will rate their subjective \u2018self-belief\u2019 in their estimated ability within each domain (visual perception, memory, general knowledge (trivia)) before and after the metacognition task battery. Participants will estimate their total accuracy on each task (0 - 100% accuracy), just before and after the metacognition battery. This will enable us to calculate an overall self-belief accuracy score, e.g. the difference between their estimated and actual total accuracy, as well as the change in this parameter after the task administration.  \n\nPost-task mind-wandering and interoception questionnaires: At the end of the study, participants will complete brief questionnaires indexing the contents of their self-generated thoughts (\u2018mind-wandering\u2019) during the testing period, and also a general questionnaire (i.e., MAIA) indexing their interoceptive beliefs (Mehling et al., 2012; Wang et al., 2018). \n\n"}, "uploader": {"extra": [], "value": ""}}}, "q18": {"value": {"question": {"extra": [], "value": ""}, "uploader": {"extra": [], "value": ""}}}, "q19": {"value": {"question": {"extra": [], "value": "Following standard procedures, we will apply a signal detection theory-based approach to estimate metacognitive bias, sensitivity, and efficiency. In brief, this approach estimates the \u2018type-II\u2019 Area Under the Receiver Operating Characteristics (AUROC2) expressing the probability of metacognitive \u2018hits\u2019 (e.g., high confidence when accurate, low confidence when inaccurate) and \u2018misses\u2019 (e.g., low confidence when accurate, high confidence when inaccurate). See Maniscalco and Lau (2012) and Fleming and Lau (2014) for a complete methodological description. \n\nUsing this approach, we will correlate metacognitive bias (average confidence), metacognitive sensitivity (meta-d\u2019), and metacognitive efficiency (m-ratio, the ratio meta-d\u2019/d\u2019) across each of the three tasks using a non-parametric bootstrapped correlation (nSamples = 10,000 per correlation) , which is robust to outliers and linearity violations (Hutson, 2019; Rousselet, Pernet, &amp; Wilcox, 2019). Statistically significant correlations will be determined as those in which the 95% bootstrapped confidence interval does not overlap zero. \n\nFor our principal hypotheses tests, we will estimate metacognition variables using the Maximum Likelihood Estimation technique developed by Maniscalco and Lau (2012). This approach is currently the most widely used in the field, and has been shown to provide reasonably good model fits when applied to tasks with more than 100 trials per condition. However, a Hierarchical Bayesian model of metacognition (HMeta-d) has recently been shown to have improved estimation even in designs with as few as 20 trials per cell. Therefor, to maximize statistical power and compare these methods, we will also test for between-task correlations on metacognitive efficiencies (m-ratio) estimated within a fully hierarchical model (Fleming, 2017; Mazancieux et al., 2018). Here, statistical significance will be assessed by calculating whether the high density interval for the between-task m-ratio correlation parameter overlaps zero. See (Fleming, 2017; Mazancieux et al., 2018) for a complete description of the hierarchical model estimation for between-task correlations. \n\n"}, "uploader": {"extra": [], "value": ""}}}, "q20": {"extra": [], "value": ""}, "q21": {"extra": [], "value": ""}, "q22": {"extra": [], "value": "Trial-preprocessing: Trials will first be cleaned for any response with RT &lt; 50 ms or those determined to be RT outliers for that condition. Missed trials (e.g., those to which a response was not provided within the allotted time) will also be removed from further analyses. Any participant for whom more than 50% of trials within a single condition are removed will also be excluded from further analysis. \n\nStudy-level exclusions: Adherence to task instructions will be checked for all participants by outlier tests on grand mean d\u2019, criterion, and confidence. Further, in the vision and trivia tasks, staircase stabilization will be assessed by visual inspection of stimulus traces and by outlier tests on the calculated threshold value. \n\nAll outlier tests will be based on the \u2018modified z-score\u2019 (Mi) proposed by Iglewicz and Hoaglin (1993). Mi will be calculated using the formula:\n\nMi=0.6745(xi-x)MAD\n\n\nWhere MAD denotes the median absolute deviation and x the median. Outliers scores will be determined as those in which the absolute value ofMi&gt; 3.5. \n\nFinally, if m-ratio cannot be estimated, for example because there is no variability in confidence reports or in accuracy, then that participant will be excluded. \n"}, "q23": {"extra": [], "value": "If a participant does not complete the full battery of tests, that participant will not be included in the analysis.\n"}, "q24": {"extra": [], "value": "Likely exploratory analyses include:\n\n1. Paired repeated measures ANOVA measuring change in self-belief (factors: pre/post, belief modality). \n\n2. Correlations between self-belief adjustment (\u2206-SelfBelief) and metacognition scores for each task. \n\n3. Fitting structural equation models or similar hierarchical models expressing covariance between type-I and type-II variables within and between each task.\n\n4. Analyzing multivariate correlations (e.g., using canonical correlation or similar) between metacognitive variables and the contents of self-reported thought (mind-wandering). \n\n5. Correlations between metacognitive ability and interoceptive awareness (MAIA) scores. \n\nAll exploratory analyses will be reported as such; some may form the basis of additional publications separate from the main confirmatory analyses. In all such cases, their relationship to this pre-registration will be made explicit. \n"}, "q25": {"extra": [], "value": "References\nBaird, B., Cieslak, M., Smallwood, J., Grafton, S. T., &amp; Schooler, J. W. (2014). Regional White Matter Variation Associated with Domain-specific Metacognitive Accuracy. Journal of Cognitive Neuroscience, 27(3), 440\u2013452. https://doi.org/10.1162/jocn_a_00741\nBaird, B., Mrazek, M. D., Phillips, D. T., &amp; Schooler, J. W. (2014). Domain-specific enhancement of metacognitive ability following meditation training. Journal of Experimental Psychology: General, 143(5), 1972\u20131979. https://doi.org/10.1037/a0036882\nBaird, B., Smallwood, J., Gorgolewski, K. J., &amp; Margulies, D. S. (2013). Medial and Lateral Networks in Anterior Prefrontal Cortex Support Metacognitive Ability for Memory and Perception. Journal of Neuroscience, 33(42), 16657\u201316665. https://doi.org/10.1523/JNEUROSCI.0786-13.2013\nCharbonnier, L., van Meer, F., van der Laan, L. N., Viergever, M. A., &amp; Smeets, P. A. M. (2016). Standardized food images: A photographing protocol and image database. Appetite, 96, 166\u2013173. https://doi.org/10.1016/j.appet.2015.08.041\nFernandez-Duque, D., Baird, J. A., &amp; Posner, M. I. (2000). Executive Attention and Metacognitive Regulation. Consciousness and Cognition, 9(2), 288\u2013307. https://doi.org/10.1006/ccog.2000.0447\nFlavell, J. H. (1979). Metacognition and cognitive monitoring: A new area of cognitive\u2013developmental inquiry. American Psychologist, 34(10), 906\u2013911. https://doi.org/10.1037/0003-066X.34.10.906\nFleming, S. M. (2017). HMeta-d: Hierarchical Bayesian estimation of metacognitive efficiency from confidence ratings. Neuroscience of Consciousness, 2017(1). https://doi.org/10.1093/nc/nix007\nFleming, S. M., &amp; Frith, C. D. (Eds.). (2014). The cognitive neuroscience of metacognition. Heidelberg: Springer.\nFleming, S. M., &amp; Lau, H. C. (2014). How to measure metacognition. Frontiers in Human Neuroscience, 8. https://doi.org/10.3389/fnhum.2014.00443\nFleming, S. M., Ryu, J., Golfinos, J. G., &amp; Blackmon, K. E. (2014). Domain-specific impairment in metacognitive accuracy following anterior prefrontal lesions. Brain, 137(10), 2811\u20132822. https://doi.org/10.1093/brain/awu221\nHutson, A. D. (2019). A robust Pearson correlation test for a general point null using a surrogate bootstrap distribution. PLOS ONE, 14(5), e0216287. https://doi.org/10.1371/journal.pone.0216287\nIglewicz, B., &amp; Hoaglin, D. C. (1993). How to detect and handle outliers (Vol. 16). Asq Press.\nManiscalco, B., &amp; Lau, H. (2012). A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings. Consciousness and Cognition, 21(1), 422\u2013430. https://doi.org/10.1016/j.concog.2011.09.021\nMazancieux, A., Fleming, S., Souchay, C., &amp; Moulin, C. (2018). Retrospective confidence judgments across tasks: Domain-general processes underlying metacognitive accuracy. https://doi.org/10.31234/osf.io/dr7ba\nMcCurdy, L. Y., Maniscalco, B., Metcalfe, J., Liu, K. Y., de Lange, F. P., &amp; Lau, H. (2013). Anatomical Coupling between Distinct Metacognitive Systems for Memory and Visual Perception. Journal of Neuroscience, 33(5), 1897\u20131906. https://doi.org/10.1523/JNEUROSCI.1890-12.2013\nMehling, W. E., Price, C., Daubenmier, J. J., Acree, M., Bartmess, E., &amp; Stewart, A. (2012). The Multidimensional Assessment of Interoceptive Awareness (MAIA). PLOS ONE, 7(11), e48230. https://doi.org/10.1371/journal.pone.0048230\nRouault, M., Dayan, P., &amp; Fleming, S. M. (2019). Forming global estimates of self-performance from local confidence. Nature Communications, 10(1), 1141. https://doi.org/10.1038/s41467-019-09075-3\nRouault, M., McWilliams, A., Allen, M. G., &amp; Fleming, S. M. (2018). Human metacognition across domains: Insights from individual differences and neuroimaging. Personality Neuroscience, 1. https://doi.org/10.1017/pen.2018.16\nRousselet, G. A., Pernet, C. R., &amp; Wilcox, R. R. (2019). The percentile bootstrap: A primer with step-by-step instructions in R [Preprint]. https://doi.org/10.31234/osf.io/kxarf\nSanders, J. I., Hangya, B., &amp; Kepecs, A. (2016). Signatures of a Statistical Computation in the Human Sense of Confidence. Neuron, 90(3), 499\u2013506. https://doi.org/10.1016/j.neuron.2016.03.025\nWang, H.-T., Poerio, G., Murphy, C., Bzdok, D., Jefferies, E., &amp; Smallwood, J. (2018). Dimensions of Experience: Exploring the Heterogeneity of the Wandering Mind. Psychological Science, 29(1), 56\u201371. https://doi.org/10.1177/0956797617728727"}}, "registration_responses": {"q1": "Modelling Domain General Metacognition: A Confirmatory Study ", "q2": "Micah Allen, Camile Correa", "q3": "Metacognition is the ability to monitor, evaluate and control one\u2019s own behavior (Fernandez-Duque, Baird, &amp; Posner, 2000; Flavell, 1979; Fleming &amp; Frith, 2014). Metacognitive ability is typically assessed by comparing subjective ratings (i.e., confidence) to objective task accuracy. Specifically, metacognition can be modelled using signal-detection theory (SDT) (Fleming &amp; Lau, 2014). This approach can quantify metacognitive performance while controlling for first-order judgement criterion, overall confidence level, and task accuracy, all of which can confound more straightforward measures of metacognition such as the raw correlation between confidence and accuracy (AKA, \u2018phi\u2019). Modelling confidence reports using SDT yields quantitative measures of metacognitive \u2018bias\u2019 (average confidence level irrespective of task performance), \u2018sensitivity\u2019 (how well subjective ratings distinguish between correct and incorrect responses\u2019), and \u2018efficiency\u2019 (metacognitive sensitivity relative to overall task performance). SDT-based measures have been applied to quantify metacognitive performance in a variety of cognitive, perceptual, and memory-related domains. Here a central question is whether metacognition is a global phenomenon, enabled by a domain-general central resource or is instead reliant on stratified domain-specific modules. For instance, McCurdy et al (2013) and Mazancieux et al (2018) both found a positive correlation between metacognitive accuracy across meta-memory and meta-perceptual tasks. However, Baird and colleagues repeatedly failed to replicate this finding (Baird, Cieslak, Smallwood, Grafton, &amp; Schooler, 2014; Baird, Mrazek, Phillips, &amp; Schooler, 2014; Baird, Smallwood, Gorgolewski, &amp; Margulies, 2013), and another study by Fleming and colleagues reported that while lesions to the rostrolateral prefrontal cortex diminished perceptual metacognition, metamemory was unimpaired (Fleming, Ryu, Golfinos, &amp; Blackmon, 2014). One recent meta-analysis found a high correlation between metacognition for different perceptual domains (i.e., auditory, tactile, or visual discrimination), but only a weak positive correlation between memory and perception (Rouault, McWilliams, Allen, &amp; Fleming, 2018). Collectively, these conflicting results highlight the substantial uncertainty regarding the question of domain generality in metacognition. In particular, to date most studies have not been pre-registered and rely on small sample sizes that may not reliably estimate the effect size of across-task correlations. Given the numerous possible researcher degrees of freedom involved in measuring metacognition, a well-powered, pre-registered confirmatory study is called for. To investigate the domain-generality (or lack thereof) of metacognitive ability, we will compare metacognitive bias, sensitivity, and efficiency across 3 different domains: vision, memory and general knowledge in up to 300 healthy participants. Additionally, in exploratory analyses we will use structural equation modelling, multivariate regression, and other techniques to explore the covariance between domain generality in metacognition, self-beliefs, and mind-wandering. \n\nAll study data, task code, and analysis code will be made available at the time of paper publication on our OSF Project Page and lab Github. \n", "q4": "Our central hypothesis tests will evaluate whether the linear correlation between metacognitive bias, sensitivity, or efficiency between any of the three tasks is greater than zero.\nIf metacognition is fully domain-specific, we expect to retain the null hypothesis of no significant correlation between the three tasks on any measure. In this case, we would expect to observe a null-bayes factor of at least 3 for one or more cross-task comparisons. \nIf metacognition is partially domain-specific, we expect to reject the null hypothesis of no significant correlation between at least two tasks on any two measures.\nIf metacognition is fully domain-general, we expect to reject the null hypothesis of no significant correlation across all three tasks, on at least one or more measures. \nThese results will strengthen or weaken the hypothesis of a common, domain general source supporting metacognitive efficiency, and shed light on what aspects of metacognition (bias, sensitivity, or efficiency) are shared versus unique across domains. \n", "q5": "Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials.", "q6": ["No blinding is involved in this study."], "q7": "No", "q9": "All subjects will perform 3 tasks. The order of the tasks will be counterbalanced across subjects, but everyone will perform the same conditions.", "q10": "Registration prior to creation of data", "q11": "Registration prior to creation of data.\n", "q13": "Our target sample size is 300 participants. We will attempt to recruit up to 330 in total, accounting for up to 10% data loss due to failure to complete the task according to the instructions, or other technical issues.", "q14": "The study will compare metacognitive performance via correlations across three different tasks in 300 subjects. According to Sch\u00f6nbrodt and Perugini (2013) a minimum of n = 150 sample is required to reliably estimate between-subject correlation coefficients. Thus far the largest prior study in this area utilized n = 181 (Mazancieux et al., 2018), and all other previous studies utilized between 30 and 60 subjects (Rouault et al., 2018). Our study thus represents a 66.7% increase in sample size over the single largest prior study, and between x4-9 times increase in sample size relative to the majority of previous investigations. A priori power analysis using G*Power shows that this sample size will provide 99.5% power to detect \u201cmedium\u201d or larger effect sizes (Cohen\u2019s \u03c1 &gt; 0.3, two-tailed exact correlation test), and a 73% power to detect \u201csmall\u201d or larger effects (Cohen\u2019s \u03c1 &gt; 0.1). Further, our study will deploy 160 more trials per metacognition condition than Mazancieux et al. (2018); as the variance of metacognitive parameter estimation is directly related to the number of trials, this will further improve our power relative to all previous studies. ", "q15": "We will stop when we finish collecting data from 330 participants, or when no more participants are available to participate. \n", "q20": "", "q21": "", "q22": "Trial-preprocessing: Trials will first be cleaned for any response with RT &lt; 50 ms or those determined to be RT outliers for that condition. Missed trials (e.g., those to which a response was not provided within the allotted time) will also be removed from further analyses. Any participant for whom more than 50% of trials within a single condition are removed will also be excluded from further analysis. \n\nStudy-level exclusions: Adherence to task instructions will be checked for all participants by outlier tests on grand mean d\u2019, criterion, and confidence. Further, in the vision and trivia tasks, staircase stabilization will be assessed by visual inspection of stimulus traces and by outlier tests on the calculated threshold value. \n\nAll outlier tests will be based on the \u2018modified z-score\u2019 (Mi) proposed by Iglewicz and Hoaglin (1993). Mi will be calculated using the formula:\n\nMi=0.6745(xi-x)MAD\n\n\nWhere MAD denotes the median absolute deviation and x the median. Outliers scores will be determined as those in which the absolute value ofMi&gt; 3.5. \n\nFinally, if m-ratio cannot be estimated, for example because there is no variability in confidence reports or in accuracy, then that participant will be excluded. \n", "q23": "If a participant does not complete the full battery of tests, that participant will not be included in the analysis.\n", "q24": "Likely exploratory analyses include:\n\n1. Paired repeated measures ANOVA measuring change in self-belief (factors: pre/post, belief modality). \n\n2. Correlations between self-belief adjustment (\u2206-SelfBelief) and metacognition scores for each task. \n\n3. Fitting structural equation models or similar hierarchical models expressing covariance between type-I and type-II variables within and between each task.\n\n4. Analyzing multivariate correlations (e.g., using canonical correlation or similar) between metacognitive variables and the contents of self-reported thought (mind-wandering). \n\n5. Correlations between metacognitive ability and interoceptive awareness (MAIA) scores. \n\nAll exploratory analyses will be reported as such; some may form the basis of additional publications separate from the main confirmatory analyses. In all such cases, their relationship to this pre-registration will be made explicit. \n", "q25": "References\nBaird, B., Cieslak, M., Smallwood, J., Grafton, S. T., &amp; Schooler, J. W. (2014). Regional White Matter Variation Associated with Domain-specific Metacognitive Accuracy. Journal of Cognitive Neuroscience, 27(3), 440\u2013452. https://doi.org/10.1162/jocn_a_00741\nBaird, B., Mrazek, M. D., Phillips, D. T., &amp; Schooler, J. W. (2014). Domain-specific enhancement of metacognitive ability following meditation training. Journal of Experimental Psychology: General, 143(5), 1972\u20131979. https://doi.org/10.1037/a0036882\nBaird, B., Smallwood, J., Gorgolewski, K. J., &amp; Margulies, D. S. (2013). Medial and Lateral Networks in Anterior Prefrontal Cortex Support Metacognitive Ability for Memory and Perception. Journal of Neuroscience, 33(42), 16657\u201316665. https://doi.org/10.1523/JNEUROSCI.0786-13.2013\nCharbonnier, L., van Meer, F., van der Laan, L. N., Viergever, M. A., &amp; Smeets, P. A. M. (2016). Standardized food images: A photographing protocol and image database. Appetite, 96, 166\u2013173. https://doi.org/10.1016/j.appet.2015.08.041\nFernandez-Duque, D., Baird, J. A., &amp; Posner, M. I. (2000). Executive Attention and Metacognitive Regulation. Consciousness and Cognition, 9(2), 288\u2013307. https://doi.org/10.1006/ccog.2000.0447\nFlavell, J. H. (1979). Metacognition and cognitive monitoring: A new area of cognitive\u2013developmental inquiry. American Psychologist, 34(10), 906\u2013911. https://doi.org/10.1037/0003-066X.34.10.906\nFleming, S. M. (2017). HMeta-d: Hierarchical Bayesian estimation of metacognitive efficiency from confidence ratings. Neuroscience of Consciousness, 2017(1). https://doi.org/10.1093/nc/nix007\nFleming, S. M., &amp; Frith, C. D. (Eds.). (2014). The cognitive neuroscience of metacognition. Heidelberg: Springer.\nFleming, S. M., &amp; Lau, H. C. (2014). How to measure metacognition. Frontiers in Human Neuroscience, 8. https://doi.org/10.3389/fnhum.2014.00443\nFleming, S. M., Ryu, J., Golfinos, J. G., &amp; Blackmon, K. E. (2014). Domain-specific impairment in metacognitive accuracy following anterior prefrontal lesions. Brain, 137(10), 2811\u20132822. https://doi.org/10.1093/brain/awu221\nHutson, A. D. (2019). A robust Pearson correlation test for a general point null using a surrogate bootstrap distribution. PLOS ONE, 14(5), e0216287. https://doi.org/10.1371/journal.pone.0216287\nIglewicz, B., &amp; Hoaglin, D. C. (1993). How to detect and handle outliers (Vol. 16). Asq Press.\nManiscalco, B., &amp; Lau, H. (2012). A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratings. Consciousness and Cognition, 21(1), 422\u2013430. https://doi.org/10.1016/j.concog.2011.09.021\nMazancieux, A., Fleming, S., Souchay, C., &amp; Moulin, C. (2018). Retrospective confidence judgments across tasks: Domain-general processes underlying metacognitive accuracy. https://doi.org/10.31234/osf.io/dr7ba\nMcCurdy, L. Y., Maniscalco, B., Metcalfe, J., Liu, K. Y., de Lange, F. P., &amp; Lau, H. (2013). Anatomical Coupling between Distinct Metacognitive Systems for Memory and Visual Perception. Journal of Neuroscience, 33(5), 1897\u20131906. https://doi.org/10.1523/JNEUROSCI.1890-12.2013\nMehling, W. E., Price, C., Daubenmier, J. J., Acree, M., Bartmess, E., &amp; Stewart, A. (2012). The Multidimensional Assessment of Interoceptive Awareness (MAIA). PLOS ONE, 7(11), e48230. https://doi.org/10.1371/journal.pone.0048230\nRouault, M., Dayan, P., &amp; Fleming, S. M. (2019). Forming global estimates of self-performance from local confidence. Nature Communications, 10(1), 1141. https://doi.org/10.1038/s41467-019-09075-3\nRouault, M., McWilliams, A., Allen, M. G., &amp; Fleming, S. M. (2018). Human metacognition across domains: Insights from individual differences and neuroimaging. Personality Neuroscience, 1. https://doi.org/10.1017/pen.2018.16\nRousselet, G. A., Pernet, C. R., &amp; Wilcox, R. R. (2019). The percentile bootstrap: A primer with step-by-step instructions in R [Preprint]. https://doi.org/10.31234/osf.io/kxarf\nSanders, J. I., Hangya, B., &amp; Kepecs, A. (2016). Signatures of a Statistical Computation in the Human Sense of Confidence. Neuron, 90(3), 499\u2013506. https://doi.org/10.1016/j.neuron.2016.03.025\nWang, H.-T., Poerio, G., Murphy, C., Bzdok, D., Jefferies, E., &amp; Smallwood, J. (2018). Dimensions of Experience: Exploring the Heterogeneity of the Wandering Mind. Psychological Science, 29(1), 56\u201371. https://doi.org/10.1177/0956797617728727", "q8.question": "The design is within subject. Participants will perform all tasks.", "q8.uploader": [], "q12.question": "Participants will be recruited through advertisements at Sona system (CobeLab) from Aarhus University and also in social media (e.g., Twitter, Facebook). Participants must be at least 18 years old, give written consent, be normal or corrected to normal vision and fluent in English. Participants will be paid 110 DKK per hour and the estimated maximum duration of the test session is 2h.", "q12.uploader": [], "q16.question": "We do not have manipulated variables. \n", "q16.uploader": [], "q17.question": "Participants will provide scores for the following 3 tasks:\n\nConfirmatory Measures:\n\nMetacognition Tasks Overview: Participants provide 2 alternative forced-choice (2AFC) responses in three psychological domains (visual perception, memory and general knowledge). On each trial, participants discriminate between the left and right stimulus using the left and right arrow keys. Participants are instructed to respond as quickly and accurately as possible. All tasks will contain 200 trials each. After each trial participants rate their confidence in the preceding decision using a 7-point likert scale. \n\nIndividual Tasks:\n\nMeta-visual task: The visual perception task will be the same as in several previous studies of metacognition (Fleming et al., 2014; Mazancieux et al., 2018; Rouault, Dayan, &amp; Fleming, 2019), code available here: https://github.com/metacoglab/meta_dots. In this 2AFC task, participants view two clouds of dots and discriminate which of the two had more total dots within it. Across trials, the difference between the two dot totals is staircased via 2 down 1 up procedure, which converges at the limit on 71% accuracy. \nMeta-memory task: The memory task is identical to that used in several previous studies (Mazancieux et al., 2018; McCurdy et al., 2013). In this task, across four blocks of fifty trials, subjects view a list of 50 words and are given either 30, 60, or 90 seconds to memorize the words. Afterwards, the memorized words are presented individually together with a distractor word from a non-memorized list, and the participant must discriminate which of the two words was seen before in a 2AFC recognition memory design. The order of the memorized vs distractor lists is counterbalanced across subjects in a full latin square design, and the order of the list study time (which is used to induce sufficient variability in discrimination accuracy) is randomized. \nMeta-knowledge \u2018Trivia\u2019 Task: This is a novel task developed for this and other studies by the current authors. The task is similar to previous investigations of metacognition which measured confidence and accuracy for \u2018domain general\u2019 or semantic knowledge tasks, e.g. about city size or other geographical facts (Mazancieux et al., 2018; Sanders, Hangya, &amp; Kepecs, 2016). A key difference is that here, participants perform \u2018trivia\u2019 judgements in multiple domains of knowledge; here, for both nutritional food and geopolitical domains (i.e., economics). In the first case, participants view two stimuli depicting 100g of food on separate plates (e.g., a plate of jelly beans and a plate of sausages). Participants must then determine which of the two plates has more total calories and rate their confidence in this choice. In the geopolitical domain, participants view flags of two countries and determine which had a higher average gross domestic product (GDP) over the previous 10 years. Crucially, and unlike previous tasks in this domain, the participant\u2019s prior experience or familiarity with each domain is titrated using an adaptive 2 down 1 up psychophysical procedure. In the case of foods, this involves adjusting the difference in calories between both food stimuli, and in the case of countries, by staircasing the log GDP difference. All food stimuli and associated caloric values are drawn from the Full4Health Image Collection (Charbonnier, van Meer, van der Laan, Viergever, &amp; Smeets, 2016). All GDP data where drawn from the World Bank Open Data library. \n\nExploratory Measures:\nSelf-belief: Participants will rate their subjective \u2018self-belief\u2019 in their estimated ability within each domain (visual perception, memory, general knowledge (trivia)) before and after the metacognition task battery. Participants will estimate their total accuracy on each task (0 - 100% accuracy), just before and after the metacognition battery. This will enable us to calculate an overall self-belief accuracy score, e.g. the difference between their estimated and actual total accuracy, as well as the change in this parameter after the task administration.  \n\nPost-task mind-wandering and interoception questionnaires: At the end of the study, participants will complete brief questionnaires indexing the contents of their self-generated thoughts (\u2018mind-wandering\u2019) during the testing period, and also a general questionnaire (i.e., MAIA) indexing their interoceptive beliefs (Mehling et al., 2012; Wang et al., 2018). \n\n", "q17.uploader": [], "q18.question": "", "q18.uploader": [], "q19.question": "Following standard procedures, we will apply a signal detection theory-based approach to estimate metacognitive bias, sensitivity, and efficiency. In brief, this approach estimates the \u2018type-II\u2019 Area Under the Receiver Operating Characteristics (AUROC2) expressing the probability of metacognitive \u2018hits\u2019 (e.g., high confidence when accurate, low confidence when inaccurate) and \u2018misses\u2019 (e.g., low confidence when accurate, high confidence when inaccurate). See Maniscalco and Lau (2012) and Fleming and Lau (2014) for a complete methodological description. \n\nUsing this approach, we will correlate metacognitive bias (average confidence), metacognitive sensitivity (meta-d\u2019), and metacognitive efficiency (m-ratio, the ratio meta-d\u2019/d\u2019) across each of the three tasks using a non-parametric bootstrapped correlation (nSamples = 10,000 per correlation) , which is robust to outliers and linearity violations (Hutson, 2019; Rousselet, Pernet, &amp; Wilcox, 2019). Statistically significant correlations will be determined as those in which the 95% bootstrapped confidence interval does not overlap zero. \n\nFor our principal hypotheses tests, we will estimate metacognition variables using the Maximum Likelihood Estimation technique developed by Maniscalco and Lau (2012). This approach is currently the most widely used in the field, and has been shown to provide reasonably good model fits when applied to tasks with more than 100 trials per condition. However, a Hierarchical Bayesian model of metacognition (HMeta-d) has recently been shown to have improved estimation even in designs with as few as 20 trials per cell. Therefor, to maximize statistical power and compare these methods, we will also test for between-task correlations on metacognitive efficiencies (m-ratio) estimated within a fully hierarchical model (Fleming, 2017; Mazancieux et al., 2018). Here, statistical significance will be assessed by calculating whether the high density interval for the between-task m-ratio correlation parameter overlaps zero. See (Fleming, 2017; Mazancieux et al., 2018) for a complete description of the hierarchical model estimation for between-task correlations. \n\n", "q19.uploader": []}, "subjects": []}, "relationships": {"children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/comments/?format=json&filter%5Btarget%5D=jm47b", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/jm47b/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/de-1/?format=json", "meta": {}}}, "data": {"id": "de-1", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/?format=json", "meta": {}}}, "data": {"id": "jm47b", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/jm47b/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/jm47b/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/jm47b/citation/?format=json", "meta": {}}}, "data": {"id": "jm47b", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/bm7jh/?format=json", "meta": {}}}, "data": {"id": "bm7jh", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/3b5nm/?format=json", "meta": {}}}, "data": {"id": "3b5nm", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5c08457ed283380029cf73bf/?format=json", "meta": {}}}, "data": {"id": "5c08457ed283380029cf73bf", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/jm47b/", "self": "https://api.osf.io/v2/registrations/jm47b/"}}, {"id": "w3s49", "type": "registrations", "attributes": {"title": "Additional files and R-code", "description": "", "category": "other", "custom_citation": "", "date_created": "2020-01-08T14:06:12.886923", "date_modified": "2020-01-30T09:36:31.896561", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": {"copyright_holders": [""], "year": "2019"}, "analytics_key": "a2c01b142504e05efdb5caf87afee457ae0b0ee3ab378c973a50efab67398d49fe23600f22ea6d0e9394261ec40adc7f324e2b2c7c83dd28d3adcf08d64e9d3443687c714ebd63fa16e9c9fc39b91d8c55b1286d035e9eebd81cd826ff82e80e7370a06bc85a44d8422ca727564a1da0ed0796f878279e0d11e4ad686f2d6577fa2b8a4ca2eff8ce02b3445182df44ae", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": "10.1111/1365-2664.13571", "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-08T14:06:12.833041", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "Open-Ended Registration", "registered_meta": {"summary": {"extra": [], "value": "This is a registered version of R-code and additional files prepared in relation to the paper \"Exploratory and confirmatory research in the open science era\" published in Journal of Applied Ecology. The link to the published paper is provided below. \n  \nThis registration contains the references (in the associated EndNote-libraries) to all papers that were available for random selection during the randomization routines. Detailed description of the routines used to perform the survey of the literature presented in Box 1, including random sampling of studies, criteria for inclusion, data extraction protocol is outlined in Appendix 1 (follow link from the published paper).  \n  \nIn addition, the registration contains an archived version of the R-code connected via a GitHub add-on. Note that the code in this registration is the version used when preparing the manuscript, and that the code in the linked GitHub repo migth evolve. \nGitHub repo: https://github.com/ErlendNilsen/NewDirectionsInConservationBiology  \n\n  \n\n"}, "uploader": {"extra": [], "value": ""}}, "registration_responses": {"summary": "This is a registered version of R-code and additional files prepared in relation to the paper \"Exploratory and confirmatory research in the open science era\" published in Journal of Applied Ecology. The link to the published paper is provided below. \n  \nThis registration contains the references (in the associated EndNote-libraries) to all papers that were available for random selection during the randomization routines. Detailed description of the routines used to perform the survey of the literature presented in Box 1, including random sampling of studies, criteria for inclusion, data extraction protocol is outlined in Appendix 1 (follow link from the published paper).  \n  \nIn addition, the registration contains an archived version of the R-code connected via a GitHub add-on. Note that the code in this registration is the version used when preparing the manuscript, and that the code in the linked GitHub repo migth evolve. \nGitHub repo: https://github.com/ErlendNilsen/NewDirectionsInConservationBiology  \n\n  \n\n"}, "subjects": []}, "relationships": {"license": {"links": {"related": {"href": "https://api.osf.io/v2/licenses/563c1cf88c5e4a3877f9e96a/?format=json", "meta": {}}}, "data": {"id": "563c1cf88c5e4a3877f9e96a", "type": "licenses"}}, "children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/comments/?format=json&filter%5Btarget%5D=w3s49", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/w3s49/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/de-1/?format=json", "meta": {}}}, "data": {"id": "de-1", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/?format=json", "meta": {}}}, "data": {"id": "w3s49", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/w3s49/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/w3s49/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/w3s49/citation/?format=json", "meta": {}}}, "data": {"id": "w3s49", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/tdfme/?format=json", "meta": {}}}, "data": {"id": "tdfme", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/n8fum/?format=json", "meta": {}}}, "data": {"id": "n8fum", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5df83f7dd28338001ac0ab0d/?format=json", "meta": {}}}, "data": {"id": "5df83f7dd28338001ac0ab0d", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/w3s49/", "self": "https://api.osf.io/v2/registrations/w3s49/"}}, {"id": "n9y7v", "type": "registrations", "attributes": {"title": "Solubility and stability of melatonin in propylene glycol, glycofurol, and dimethyl sulfoxide", "description": "", "category": "project", "custom_citation": "", "date_created": "2020-01-30T05:53:28.756552", "date_modified": "2020-01-30T06:03:29.051918", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": {"copyright_holders": [""], "year": "2020"}, "analytics_key": "805d0f7399ce9fc3831ba03db831c18456b48e738a8068345f1b04ef07fe4154711f8a8a10bce93a7770b391262af3994a77e108cbe0d14cc5396651c305f46272b23b79e2d882016695fcce47cfceab217846ef8b996dab555a35410e4c7a1b9dc880b96f4932b0f4788fd151f8aed3b3fed251aff2c1ff84123257444f5b0c05c85a9dc1639ff1d3728f1ee02957d7", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-30T05:53:28.731568", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "Open-Ended Registration", "registered_meta": {"summary": {"extra": [], "value": "This registration contains all data related to the study titled \"Solubility and stability of melatonin in propylene glycol, glycofurol, and dimethyl sulfoxide\"."}, "uploader": {"extra": [{"data": {"name": "Appendix I.docx"}, "nodeId": "8gk3y", "sha256": "7151a8a909aa6cbaafbad57532a4f3775f3d3e2d4680153aec728e3e9011e9a5", "viewUrl": "/project/8gk3y/files/osfstorage/5e17099a75458402dbf9f030", "selectedFileName": "Appendix I.docx"}, {"data": {"name": "Data.xlsx"}, "nodeId": "8gk3y", "sha256": "2706a82de26af2117e67f65dbee29974f28b33ea0fb7af8eb9a1e14ad5b8a44b", "viewUrl": "/project/8gk3y/files/osfstorage/5e1709a575458402d6fa3a3f", "selectedFileName": "Data.xlsx"}, {"data": {"name": "Chromatograms.xlsx"}, "nodeId": "8gk3y", "sha256": "60add649b6b644267c13aa47bc8acac18d5ca2bd2df2022aa1329ad4314c6122", "viewUrl": "/project/8gk3y/files/osfstorage/5e3167c3e71ef800de18b8c6", "selectedFileName": "Chromatograms.xlsx"}], "value": ""}}, "registration_responses": {"summary": "This registration contains all data related to the study titled \"Solubility and stability of melatonin in propylene glycol, glycofurol, and dimethyl sulfoxide\".", "uploader": [{"file_id": "5e17099a75458402dbf9f030", "file_name": "Appendix I.docx", "file_urls": {"html": "https://osf.io/8gk3y/files/osfstorage/5e17099a75458402dbf9f030", "download": "https://osf.io/download/3qvmp/?view_only=525447a8deb44d67a6599fa874946ea3"}, "file_hashes": {"sha256": "7151a8a909aa6cbaafbad57532a4f3775f3d3e2d4680153aec728e3e9011e9a5"}}, {"file_id": "5e1709a575458402d6fa3a3f", "file_name": "Data.xlsx", "file_urls": {"html": "https://osf.io/8gk3y/files/osfstorage/5e1709a575458402d6fa3a3f", "download": "https://osf.io/download/pt3j5/?view_only=525447a8deb44d67a6599fa874946ea3"}, "file_hashes": {"sha256": "2706a82de26af2117e67f65dbee29974f28b33ea0fb7af8eb9a1e14ad5b8a44b"}}, {"file_id": "5e3167c3e71ef800de18b8c6", "file_name": "Chromatograms.xlsx", "file_urls": {"html": "https://osf.io/8gk3y/files/osfstorage/5e3167c3e71ef800de18b8c6", "download": "https://osf.io/download/sw6a8/?view_only=525447a8deb44d67a6599fa874946ea3"}, "file_hashes": {"sha256": "60add649b6b644267c13aa47bc8acac18d5ca2bd2df2022aa1329ad4314c6122"}}]}, "subjects": []}, "relationships": {"license": {"links": {"related": {"href": "https://api.osf.io/v2/licenses/563c1cf88c5e4a3877f9e96c/?format=json", "meta": {}}}, "data": {"id": "563c1cf88c5e4a3877f9e96c", "type": "licenses"}}, "children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/comments/?format=json&filter%5Btarget%5D=n9y7v", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/n9y7v/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/de-1/?format=json", "meta": {}}}, "data": {"id": "de-1", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/?format=json", "meta": {}}}, "data": {"id": "n9y7v", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/n9y7v/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/n9y7v/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/n9y7v/citation/?format=json", "meta": {}}}, "data": {"id": "n9y7v", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/tm3qc/?format=json", "meta": {}}}, "data": {"id": "tm3qc", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/8gk3y/?format=json", "meta": {}}}, "data": {"id": "8gk3y", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5df83f7dd28338001ac0ab0d/?format=json", "meta": {}}}, "data": {"id": "5df83f7dd28338001ac0ab0d", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/n9y7v/", "self": "https://api.osf.io/v2/registrations/n9y7v/"}}, {"id": "t56ep", "type": "registrations", "attributes": {"title": "From a Terror Management Perspective: The Efficacy of Self-Affirmation on  Organ Donation-Related Thoughts and Intentions in China", "description": "", "category": "project", "custom_citation": "", "date_created": "2020-01-29T05:59:26.413629", "date_modified": "2020-01-30T05:33:11.185427", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": null, "analytics_key": "825f7fe3de8dca312a0eec8bd9f29a088ca1554e625378b7d622f9b60079ddceb8a1a68e704f7c93f733d7719ed7bf846cbd7511285cab859da602316291fc5326ddbac6978f760f74ceafbae1443446626b709f33db89eff8c11e52126b7796394d118ee850c330cf76a29df8270fee3d4ef34da14df97954a38d9e282654287c44accc6528cefebb5f47e7ccdf5db0", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-29T05:59:26.396747", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "Open-Ended Registration", "registered_meta": {"summary": {"extra": [], "value": "This registration contains the experimental procedure, questionnaire items, a data file, and syntax and result files for \"From a Terror Management Perspective: The Efficacy of Self-Affirmation on Organ Donation-Related Thoughts and Intentions in China\" published by Journal of Social Psychology.  "}, "uploader": {"extra": [], "value": ""}}, "registration_responses": {"summary": "This registration contains the experimental procedure, questionnaire items, a data file, and syntax and result files for \"From a Terror Management Perspective: The Efficacy of Self-Affirmation on Organ Donation-Related Thoughts and Intentions in China\" published by Journal of Social Psychology.  "}, "subjects": []}, "relationships": {"children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/comments/?format=json&filter%5Btarget%5D=t56ep", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/t56ep/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/us/?format=json", "meta": {}}}, "data": {"id": "us", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/?format=json", "meta": {}}}, "data": {"id": "t56ep", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/t56ep/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/t56ep/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/t56ep/citation/?format=json", "meta": {}}}, "data": {"id": "t56ep", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/usy2m/?format=json", "meta": {}}}, "data": {"id": "usy2m", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/h5cyp/?format=json", "meta": {}}}, "data": {"id": "h5cyp", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5df83f7dd28338001ac0ab0d/?format=json", "meta": {}}}, "data": {"id": "5df83f7dd28338001ac0ab0d", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/t56ep/", "self": "https://api.osf.io/v2/registrations/t56ep/"}}, {"id": "49v8w", "type": "registrations", "attributes": {"title": "Syntactic disambiguation through prosodic cues by young Brazilian Portuguese-learning children", "description": "", "category": "project", "custom_citation": "", "date_created": "2020-01-23T19:04:27.878608", "date_modified": "2020-01-30T05:00:56.389476", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": null, "analytics_key": "a68dda3430df50684e4be8baf9274af22718af8ce1a9fc0059b75b8020b6e30100c48be951da1bbdfe1b480916733dd773267d58aee5e0106a16b57cc486050031b5b9d145269b066ca3e0c82136d20d0cfa359cfa452bd53159af39380b4447dbb7f26ec85963740a7f1ba1d35dcb48de85da354baf21417a472d795e8f5fd349fcdef7f07405852e61dbab3734882e", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-23T19:04:27.840543", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "OSF Preregistration", "registered_meta": {"q1": {"extra": [], "value": "Comprehension of stripping sentences through prosodic boundary information by Brazilian Portuguese learning children"}, "q2": {"extra": [], "value": "Leticia Schiavon Kolberg, Anne CHRISTOPHE, Alex de Carvalho, Maria Bernadete Marques Abaurre, Naomi Havron, Mireille Babineau"}, "q3": {"extra": [], "value": "This work aims to investigate if Brazilian Portuguese (BP) learning children correctly interpret stripping sentences (a kind of Tense Phrase ellipsis) such as \"O tigre t\u00e1 cutucando. O pato tamb\u00e9m\" (\"The tiger is poking. The duck too\"), interpreted as if the tiger and the duck are performing a poking action relying mainly on the prosodic boundary information which tells them apart from sentences such as \"The tiger is poking the duck too\", interpreted as  if the tiger is poking the duck. Previous work has shown that French children can tell apart these two types of sentences when presented to an ambiguous semantic context, where they see two videos side by side, each giving the interpretation of one of the type of sentences above. We tested 3-4-year-old BP-learning children with the same methodology, aiming to investigate if they could also tell apart these types of sentences based solely on their prosodic boundary cues."}, "q4": {"extra": [], "value": "As French-learning children, BP-learning children can also use the prosodic boundary cues in stripping sentences to interpret them differently from the simple transitive sentences, and so they should look longer/point more toward the two-agents action when exposed to stripping sentences than when exposed to simple transitive sentences. If, however, they ignore the prosodic boundary information in stripping sentences, they should inter-pret them as simple transitive sentences and look longer/point more toward the transitive ac-tion. The predictions for the results are the same as for the French experiment. "}, "q5": {"extra": [], "value": "Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials."}, "q6": {"extra": [], "value": ["For studies that involve human subjects, they will not know the treatment group to which they have been assigned."]}, "q7": {"extra": [], "value": ""}, "q8": {"value": {"question": {"extra": [], "value": "We have a between subjects design with two groups (stripping sentences versus intransitive sentences). Each group sees four test trials with four known actions (eat, poke, carry, and push) and a training trial. The only difference between the groups are the test sentences. The first part of the experiment is the character introduction phase, where five puppet animal characters appear waving and dancing alone on the screen for five seconds each, while the narrator names them in sentences such as \u201cOlha! Um macaco!\u201d (Look! a monkey!). Afterwards, the training phase begins. The two video stimuli for this phase are a bunny jumping, and a bunny and a monkey playing. First, each video appears separately on the screen, and then at the same time, accompanied by attention-getters such as \u201cOlha! T\u00e1 vendo isso?\u201d (Look! Do you see that?).  Then, the videos disappear, and the target sentence is played (\u201cOlha s\u00f3! O coelho vai pular\u201d) (Look! The bunny is going to jump!). After that, the videos show up one more time on the screen for 12 seconds, accompanied by two repetitions of the target sentence in the present tense (\u201cO coelho vai pular!\u201d). At the end of the 12 seconds, the videos freeze, and the experimenter asks children which video the lady was talking about. The pointing responses are then recorded by the experimenter through a keyboard.\nThe training phase is followed by four test trials, with the verbs \u201ceat\u201d (a tiger eats a duck and a dinosaur with a fork/ the dinosaur and the tiger eat the duck with forks), \u201cpoke\u201d  (a tiger pokes a bunny and a duck with a stick/ the tiger and the duck poke the bunny with sticks), \u201cpush\u201d (a monkey pushes a trolley cart with a bunny and a dinosaur/ a bunny and a monkey push a trolley with a dinosaur) and \u201ccarry\u201d (a duck carries a present box and a dinosaur/ a duck and a dinosaur carry a present box each). The first part of each test phase consists of presenting the child with the context for the target sentence. For instance, for the \"eat\" sentences, the tiger appears first eating only a duck, and the child hears the sentence \"Olha! O tigre! Ele t\u00e1 cutucando!\" (Look! It's the tiger! He is poking!). This is followed by the preview and the test trial, which follow the same pattern as in the training phase. The experiment takes about 10 minutes, including the time for the children's pointing responses."}, "uploader": {"extra": [{"data": {"name": "Experiment1_BP_design.png"}, "nodeId": "98cmu", "sha256": "150d40351cb9eb86610a133fd88851a55de064393b8774092576622f5cd7d0ab", "viewUrl": "/project/49v8w/files/osfstorage/5e29ee4187a1d900781cd2bc/", "selectedFileName": "Experiment1_BP_design.png"}], "value": ""}}}, "q9": {"extra": [], "value": "To avoid possible order biases in children's responses , we created four complete videos per condition, alternating the order of appearance of the test trials in each video so that all four test trials appeared first in one video, second in another one, and so on . The left-right disposition of the videos was also randomized so children did not see more than two consecutive target videos on the same side. Children were assigned a particular version of the test by order of testing: the first and second child saw the first and second versions of the stripping videos; the third and fourth saw the first and second versions of the transitive videos; the fifth and sixth saw the third and fourth versions of the stripping videos, and so on. After 70% of the children have been tested, we checked if the groups were balanced (they become unbalanced when we exclude children or children opt to leave at the middle of the task) and re-assigned the remaining children as necessary."}, "q10": {"extra": [], "value": "Registration following analysis of the data"}, "q11": {"extra": [], "value": "This experiment is a replication of a preregistered French experiment, and as such, it follows the same design, data collection procedures, hypotheses and analysis plan as the previous one. The preregistration of the French experiment is available through the following link: https://osf.io/5r97t"}, "q12": {"value": {"question": {"extra": [], "value": "The subjects will be 48 children from 36 to 50 months old, and 48 children from 28 to 30 months old. The participants are recruited via an invitation letter from the experimenter, delivered by the teachers from the daycares. Parents had to sign a consent form before testing begins. We did not include children with language or learning disabilities and children who were exposed to more than 20% of the time to languages other than Brazilian Portuguese at home. Children who were fussy or distracted during the experiment were also excluded from the analysis."}, "uploader": {"extra": [], "value": ""}}}, "q13": {"extra": [], "value": "The subjects will be 48 children from 36 to 50 months old, and 48 children from 28 to 30 months old. Within each age group, half of the children will be in the transitive condition, and the other half will be in the stripping condition. Children are invited to come to the lab in batches, it may happen that a few more appointments have been taken in advance, when the 48th child is tested; in that case we will test all remaining participants and include them in the data analysis (we may thus have slightly more than 48 children in the final sample)."}, "q14": {"extra": [], "value": "Our previous study, which had a similar design and a sample size of 51 (n = 25 for group 1, and n = 26 for group 2) obtained a large effect size (t = 4.92; p &lt; .001; Cohen's d = 1.36), with a 99% power."}, "q15": {"extra": [], "value": ""}, "q16": {"value": {"question": {"extra": [], "value": "For half of the children, all test sentences were simple transitive sentences, and for the other half, all test sentences were stripping sentences. We randomly assigned one of the two experimental conditions to each child. The training phase is the same for all children, and the correct video is always the jumping video. The order of presentation of the trials is randomized, and the left-right distribution of the videos on the screen is semi-randomized in a way that the target video in one trial is always on the opposite side from the next trial's target, except for the training trial, in which the target video is always on the same side as the target of the next trial. This ensures the target videos for the test phase appear the same amount of time on each side, and that there are no more than two consecutive target videos on the same side."}, "uploader": {"extra": [{"data": {"name": "Randomization_BP_experiment.png"}, "nodeId": "98cmu", "sha256": "7bb1d8f6369bda6660d36087ec70a37447eea3ce78d528a68f053ca053077596", "viewUrl": "/project/49v8w/files/osfstorage/5e29ee4187a1d900781cd2ba/", "selectedFileName": "Randomization_BP_experiment.png"}], "value": ""}}}, "q17": {"value": {"question": {"extra": [], "value": "The measured variables for analysis are the time children spend looking at each of the videos, and the pointing responses of the older children. Because the proportion of looks toward the stripping and the transitive videos are complementary (with exception of time spent looking away), we'll consider the proportion of looks toward the stripping video. For the pointing responses, the dependent variable is the proportion of pointing to the stripping videos. "}, "uploader": {"extra": [], "value": ""}}}, "q18": {"value": {"question": {"extra": [], "value": "For each participant, we measure the time course of their eye-gaze during each test trial. Looking time will be recorded by an eye-tracker every 2ms, but in our analyses we will average the data in 170-ms bins."}, "uploader": {"extra": [], "value": ""}}}, "q19": {"value": {"question": {"extra": [], "value": "For the looking data, We conducted a cluster-based permutation analysis (Maris &amp; Oostenveld, 2007), to find the time-window(s) where a significant effect of condition was observed, indicating that children looked more towards the stripping video when listening to the stripping sentences than when listening to the transitive sentences. Such an effect may appear towards the offset of the sentence, since for this type of sentences, the crucial information (who's undergoing the action, for the patient of the transitive sentence, or who\u2019s also performing an action, for the agent of the second VP on the stripping sentence) is at the end. However, since children heard the target sentence once before seeing the videos and saw the videos before being presented to the target sentence (so they know what video to expect on each side of the screen), it is possible that they show preference to the video corresponding to the heard sentence from the beginning of the trial. Therefore, we applied the cluster-based analysis  to the whole duration of the trial.  Adjacent time points with a t-value greater than t = 1.5 (for the comparison between groups) were grouped together into clusters, and the probability of observing a cluster of the same size by chance was estimated by running the same analysis on simulated data (1000 iterations) in which groups were randomly assigned to participants. \nWe also compared looking times for the duration of the entire trials by using a t-test, to analyze the possibility that the general proportion of looks toward the stripping video is significantly bigger for the stripping group than for the transitive group.\nFor the pointing responses, the dependent variable was the proportion of pointing towards the stripping videos. If children from the studied age range can use the prosodic information of the sentences in order to figure out it's syntactic structure, then they should point more towards the stripping videos when presented with stripping sentences, and more towards the transitive videos when presented with transitive sentences. To confirm this, we ran a mixed-effects regression with pointing towards stripping vs. transitive pointing as the dependent variable, condition as a fixed effect, participants as a random effect with a random slope for condition (which was the maximal random effect structure  that allowed the model to converge)"}, "uploader": {"extra": [], "value": ""}}}, "q20": {"extra": [], "value": ""}, "q21": {"extra": [], "value": "Following most infant eyetracking studies, we will use the standard p &lt; .05 criteria for determining if there is a significant difference of proportion of looks toward the stripping videos between the groups.\n\n"}, "q22": {"extra": [], "value": "We did not test children with language or learning disabilities and children who were exposed to more than 20% of the time to languages other than Brazilian Portuguese at home. Other exclusion criteria were fussiness during the experiment, crying and interference of parents during the task. These last criteria were evaluated by the experimenter during the experiment, or by watching the children's videos without sound (so the experimenter was blind to the condition the child was in). Test trials were excluded from analysis if there was more than 25% of eye-tracking data missing. If a participant had more than two excluded test trials (out of 4), the participant's eye data was excluded from the analysis. If, however, the video or the experimenter's notes indicated the child had no other reason to be left out of the experiment, the pointing data was kept for analysis (since eye data loss may also be related to failure to calibrate or to finding the child's eye by the eye-tracker). \n\n"}, "q23": {"extra": [], "value": "Eye-tracking missing data was excluded from our analysis since we expected that the proportion of missing data in each condition would not be significantly different. "}, "q24": {"extra": [], "value": "Since we found a significant time cluster which extended until the end of the test trial, we also performed the analysis considering the time until 500ms after the end of the trial, to see how far this cluster would go. At the end of each test trial, the videos froze on the screen for the pointing trial, so children could still see the two frozen videos for more than 5000ms (since the experimenter also paused the videos to ask children to point).\nWe also intend to compare children's looking pattern on the test trials with their looking pattern during the contrast phase, which showed the test videos previous to the exposure of the test sentence. We will run a mixed effects model with the maximal random effect structure that allows the model to converge, starting with condition and phase (contrast vs. test) as fixed effects, participants as random intercept, and condition as a random effect."}, "q25": {"extra": [], "value": ""}}, "registration_responses": {"q1": "Comprehension of stripping sentences through prosodic boundary information by Brazilian Portuguese learning children", "q2": "Leticia Schiavon Kolberg, Anne CHRISTOPHE, Alex de Carvalho, Maria Bernadete Marques Abaurre, Naomi Havron, Mireille Babineau", "q3": "This work aims to investigate if Brazilian Portuguese (BP) learning children correctly interpret stripping sentences (a kind of Tense Phrase ellipsis) such as \"O tigre t\u00e1 cutucando. O pato tamb\u00e9m\" (\"The tiger is poking. The duck too\"), interpreted as if the tiger and the duck are performing a poking action relying mainly on the prosodic boundary information which tells them apart from sentences such as \"The tiger is poking the duck too\", interpreted as  if the tiger is poking the duck. Previous work has shown that French children can tell apart these two types of sentences when presented to an ambiguous semantic context, where they see two videos side by side, each giving the interpretation of one of the type of sentences above. We tested 3-4-year-old BP-learning children with the same methodology, aiming to investigate if they could also tell apart these types of sentences based solely on their prosodic boundary cues.", "q4": "As French-learning children, BP-learning children can also use the prosodic boundary cues in stripping sentences to interpret them differently from the simple transitive sentences, and so they should look longer/point more toward the two-agents action when exposed to stripping sentences than when exposed to simple transitive sentences. If, however, they ignore the prosodic boundary information in stripping sentences, they should inter-pret them as simple transitive sentences and look longer/point more toward the transitive ac-tion. The predictions for the results are the same as for the French experiment. ", "q5": "Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials.", "q6": ["For studies that involve human subjects, they will not know the treatment group to which they have been assigned."], "q7": "", "q9": "To avoid possible order biases in children's responses , we created four complete videos per condition, alternating the order of appearance of the test trials in each video so that all four test trials appeared first in one video, second in another one, and so on . The left-right disposition of the videos was also randomized so children did not see more than two consecutive target videos on the same side. Children were assigned a particular version of the test by order of testing: the first and second child saw the first and second versions of the stripping videos; the third and fourth saw the first and second versions of the transitive videos; the fifth and sixth saw the third and fourth versions of the stripping videos, and so on. After 70% of the children have been tested, we checked if the groups were balanced (they become unbalanced when we exclude children or children opt to leave at the middle of the task) and re-assigned the remaining children as necessary.", "q10": "Registration following analysis of the data", "q11": "This experiment is a replication of a preregistered French experiment, and as such, it follows the same design, data collection procedures, hypotheses and analysis plan as the previous one. The preregistration of the French experiment is available through the following link: https://osf.io/5r97t", "q13": "The subjects will be 48 children from 36 to 50 months old, and 48 children from 28 to 30 months old. Within each age group, half of the children will be in the transitive condition, and the other half will be in the stripping condition. Children are invited to come to the lab in batches, it may happen that a few more appointments have been taken in advance, when the 48th child is tested; in that case we will test all remaining participants and include them in the data analysis (we may thus have slightly more than 48 children in the final sample).", "q14": "Our previous study, which had a similar design and a sample size of 51 (n = 25 for group 1, and n = 26 for group 2) obtained a large effect size (t = 4.92; p &lt; .001; Cohen's d = 1.36), with a 99% power.", "q15": "", "q20": "", "q21": "Following most infant eyetracking studies, we will use the standard p &lt; .05 criteria for determining if there is a significant difference of proportion of looks toward the stripping videos between the groups.\n\n", "q22": "We did not test children with language or learning disabilities and children who were exposed to more than 20% of the time to languages other than Brazilian Portuguese at home. Other exclusion criteria were fussiness during the experiment, crying and interference of parents during the task. These last criteria were evaluated by the experimenter during the experiment, or by watching the children's videos without sound (so the experimenter was blind to the condition the child was in). Test trials were excluded from analysis if there was more than 25% of eye-tracking data missing. If a participant had more than two excluded test trials (out of 4), the participant's eye data was excluded from the analysis. If, however, the video or the experimenter's notes indicated the child had no other reason to be left out of the experiment, the pointing data was kept for analysis (since eye data loss may also be related to failure to calibrate or to finding the child's eye by the eye-tracker). \n\n", "q23": "Eye-tracking missing data was excluded from our analysis since we expected that the proportion of missing data in each condition would not be significantly different. ", "q24": "Since we found a significant time cluster which extended until the end of the test trial, we also performed the analysis considering the time until 500ms after the end of the trial, to see how far this cluster would go. At the end of each test trial, the videos froze on the screen for the pointing trial, so children could still see the two frozen videos for more than 5000ms (since the experimenter also paused the videos to ask children to point).\nWe also intend to compare children's looking pattern on the test trials with their looking pattern during the contrast phase, which showed the test videos previous to the exposure of the test sentence. We will run a mixed effects model with the maximal random effect structure that allows the model to converge, starting with condition and phase (contrast vs. test) as fixed effects, participants as random intercept, and condition as a random effect.", "q25": "", "q8.question": "We have a between subjects design with two groups (stripping sentences versus intransitive sentences). Each group sees four test trials with four known actions (eat, poke, carry, and push) and a training trial. The only difference between the groups are the test sentences. The first part of the experiment is the character introduction phase, where five puppet animal characters appear waving and dancing alone on the screen for five seconds each, while the narrator names them in sentences such as \u201cOlha! Um macaco!\u201d (Look! a monkey!). Afterwards, the training phase begins. The two video stimuli for this phase are a bunny jumping, and a bunny and a monkey playing. First, each video appears separately on the screen, and then at the same time, accompanied by attention-getters such as \u201cOlha! T\u00e1 vendo isso?\u201d (Look! Do you see that?).  Then, the videos disappear, and the target sentence is played (\u201cOlha s\u00f3! O coelho vai pular\u201d) (Look! The bunny is going to jump!). After that, the videos show up one more time on the screen for 12 seconds, accompanied by two repetitions of the target sentence in the present tense (\u201cO coelho vai pular!\u201d). At the end of the 12 seconds, the videos freeze, and the experimenter asks children which video the lady was talking about. The pointing responses are then recorded by the experimenter through a keyboard.\nThe training phase is followed by four test trials, with the verbs \u201ceat\u201d (a tiger eats a duck and a dinosaur with a fork/ the dinosaur and the tiger eat the duck with forks), \u201cpoke\u201d  (a tiger pokes a bunny and a duck with a stick/ the tiger and the duck poke the bunny with sticks), \u201cpush\u201d (a monkey pushes a trolley cart with a bunny and a dinosaur/ a bunny and a monkey push a trolley with a dinosaur) and \u201ccarry\u201d (a duck carries a present box and a dinosaur/ a duck and a dinosaur carry a present box each). The first part of each test phase consists of presenting the child with the context for the target sentence. For instance, for the \"eat\" sentences, the tiger appears first eating only a duck, and the child hears the sentence \"Olha! O tigre! Ele t\u00e1 cutucando!\" (Look! It's the tiger! He is poking!). This is followed by the preview and the test trial, which follow the same pattern as in the training phase. The experiment takes about 10 minutes, including the time for the children's pointing responses.", "q8.uploader": [{"file_id": "5e29ee4187a1d900781cd2bc", "file_name": "Experiment1_BP_design.png", "file_urls": {"html": "https://osf.io/project/49v8w/files/osfstorage/5e29ee4187a1d900781cd2bc", "download": "https://osf.io/download/5e29ee4187a1d900781cd2bc"}, "file_hashes": {"sha256": "150d40351cb9eb86610a133fd88851a55de064393b8774092576622f5cd7d0ab"}}], "q12.question": "The subjects will be 48 children from 36 to 50 months old, and 48 children from 28 to 30 months old. The participants are recruited via an invitation letter from the experimenter, delivered by the teachers from the daycares. Parents had to sign a consent form before testing begins. We did not include children with language or learning disabilities and children who were exposed to more than 20% of the time to languages other than Brazilian Portuguese at home. Children who were fussy or distracted during the experiment were also excluded from the analysis.", "q12.uploader": [], "q16.question": "For half of the children, all test sentences were simple transitive sentences, and for the other half, all test sentences were stripping sentences. We randomly assigned one of the two experimental conditions to each child. The training phase is the same for all children, and the correct video is always the jumping video. The order of presentation of the trials is randomized, and the left-right distribution of the videos on the screen is semi-randomized in a way that the target video in one trial is always on the opposite side from the next trial's target, except for the training trial, in which the target video is always on the same side as the target of the next trial. This ensures the target videos for the test phase appear the same amount of time on each side, and that there are no more than two consecutive target videos on the same side.", "q16.uploader": [{"file_id": "5e29ee4187a1d900781cd2ba", "file_name": "Randomization_BP_experiment.png", "file_urls": {"html": "https://osf.io/project/49v8w/files/osfstorage/5e29ee4187a1d900781cd2ba", "download": "https://osf.io/download/5e29ee4187a1d900781cd2ba"}, "file_hashes": {"sha256": "7bb1d8f6369bda6660d36087ec70a37447eea3ce78d528a68f053ca053077596"}}], "q17.question": "The measured variables for analysis are the time children spend looking at each of the videos, and the pointing responses of the older children. Because the proportion of looks toward the stripping and the transitive videos are complementary (with exception of time spent looking away), we'll consider the proportion of looks toward the stripping video. For the pointing responses, the dependent variable is the proportion of pointing to the stripping videos. ", "q17.uploader": [], "q18.question": "For each participant, we measure the time course of their eye-gaze during each test trial. Looking time will be recorded by an eye-tracker every 2ms, but in our analyses we will average the data in 170-ms bins.", "q18.uploader": [], "q19.question": "For the looking data, We conducted a cluster-based permutation analysis (Maris &amp; Oostenveld, 2007), to find the time-window(s) where a significant effect of condition was observed, indicating that children looked more towards the stripping video when listening to the stripping sentences than when listening to the transitive sentences. Such an effect may appear towards the offset of the sentence, since for this type of sentences, the crucial information (who's undergoing the action, for the patient of the transitive sentence, or who\u2019s also performing an action, for the agent of the second VP on the stripping sentence) is at the end. However, since children heard the target sentence once before seeing the videos and saw the videos before being presented to the target sentence (so they know what video to expect on each side of the screen), it is possible that they show preference to the video corresponding to the heard sentence from the beginning of the trial. Therefore, we applied the cluster-based analysis  to the whole duration of the trial.  Adjacent time points with a t-value greater than t = 1.5 (for the comparison between groups) were grouped together into clusters, and the probability of observing a cluster of the same size by chance was estimated by running the same analysis on simulated data (1000 iterations) in which groups were randomly assigned to participants. \nWe also compared looking times for the duration of the entire trials by using a t-test, to analyze the possibility that the general proportion of looks toward the stripping video is significantly bigger for the stripping group than for the transitive group.\nFor the pointing responses, the dependent variable was the proportion of pointing towards the stripping videos. If children from the studied age range can use the prosodic information of the sentences in order to figure out it's syntactic structure, then they should point more towards the stripping videos when presented with stripping sentences, and more towards the transitive videos when presented with transitive sentences. To confirm this, we ran a mixed-effects regression with pointing towards stripping vs. transitive pointing as the dependent variable, condition as a fixed effect, participants as a random effect with a random slope for condition (which was the maximal random effect structure  that allowed the model to converge)", "q19.uploader": []}, "subjects": []}, "relationships": {"children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/comments/?format=json&filter%5Btarget%5D=49v8w", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/49v8w/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/us/?format=json", "meta": {}}}, "data": {"id": "us", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/?format=json", "meta": {}}}, "data": {"id": "49v8w", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/49v8w/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/49v8w/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/49v8w/citation/?format=json", "meta": {}}}, "data": {"id": "49v8w", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/63q9r/?format=json", "meta": {}}}, "data": {"id": "63q9r", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/98cmu/?format=json", "meta": {}}}, "data": {"id": "98cmu", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5c08457ed283380029cf73bf/?format=json", "meta": {}}}, "data": {"id": "5c08457ed283380029cf73bf", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/49v8w/", "self": "https://api.osf.io/v2/registrations/49v8w/"}}, {"id": "utb2e", "type": "registrations", "attributes": {"title": "Effects of load and spatial attention on neuronal responses to fearful and neutral faces", "description": "", "category": "project", "custom_citation": "", "date_created": "2020-01-16T09:35:45.424905", "date_modified": "2020-01-30T05:00:42.772952", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": null, "analytics_key": "4add980aab7f530558f0736ef18bdd410c1906df743b11081575ca962847eca3a4eee38d722b077bbdaf4550830d5dab46e2302d93605c7c219b590b1991048d7feb0a36f8564dbcc2879db2dde18b3ef3ddd8139856364aa92bd1325cdc1cd736d95b48ae5058ab003617f9c4e5787bc11f20804c1854844380c8d8bcc55b1cf125e49eba835c55293cd81200b274f3", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-16T09:35:45.402031", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "Preregistration Template from AsPredicted.org", "registered_meta": {"data": {"extra": [], "value": "No, no data have been collected for this study yet."}, "name": {"extra": [], "value": "Effects of load and spatial attention on neuronal responses to fearful and neutral faces"}, "other": {"extra": [], "value": "To ensure that participants shift their attention to the instructed left and right locations, power spectra will be computed by means of a Fast Fourier Transform of single-trial raw data in a one-second long pre-stimulus time window and subsequent averaging across trials (see above). Further, we will compare alpha activity for blocks with high versus low perceptual difficulty."}, "sample": {"extra": [], "value": "Participants. In total, the data sampling plan is designed to examine 40 participants, as power calculations using G*Power 3.1.7 (Faul et al. 2009) show that sampling 40 participants leads to a power of &gt; 90% to detect medium effects sizes (f = 0.25), for a within two by three repeated-measure ANOVA designs. "}, "analyses": {"extra": [], "value": "Data analyses. Behavioural data will be analysed with JASP (www.jasp.org/), using a two (Load: high vs low) by three level (Attention location: left, central, right) repeated measure ANOVA, to investigate main effects of task load and attentional location as secondary analyses. \nEEG scalp-data will be statistically analyzed with EMEGS. Here, for each ERP component, a two (Emotion: fearful, neutral) by two (Load: high vs low) by three (Attention location: left, central, right) repeated measure ANOVA is used investigate main effects of attentional location and emotional expression and, as well as their interaction. Partial eta-squared (partial \u03b72) will be used to describe effect sizes, where \u03b7P2 = 0.02 describes a small, \u03b7P2 = 0.13 a medium and \u03b7P2 = 0.26 a large effect (Cohen, 1988). For the EPN and LPP, typically scored as differences between emotional and neutral stimuli, all task conditions will be collapsed to identify emotion effects. Time windows will be approximately segmented from 80 to 100 ms for the P1, 130 to 170 ms for the N170, from 200 to 350 ms to investigate EPN effects, and from 400 to 600 ms to investigate LPP effects. Based on previous pre-registered studies using faces across tasks (Bruchmann et al. in preparation; Schindler, Bruchmann, Bublatzky, et al. 2019; Schindler, Bruchmann, Gathmann, et al. 2019), we expect to score for the P1, N170, and EPN time window over two symmetrical occipital clusters (left P9, P7, PO7; right P10, P8, PO8). However, please note, that this can be due to task effects shifted either to more temporal or occipital regions, which will be examined by the above-mentioned collapsed localizer. \nTo ensure that participants shift their attention to the instructed location presentation, power spectra will be computed by means of a Fast Fourier Transform of single-trial raw data in a one-second long pre-stimulus time window and subsequent averaging across trials. To account for interindividual variability in topography and frequency (Haegens et al. 2014) of the alpha rhythm, we will identify individual peak channels and peak frequencies. Peak frequencies will be defined as the frequency with the strongest power within a 5 \u2013 15 Hz range. Peak channels will be defined as the set of 10 posterior channels showing the strongest power at the peak frequency. Alpha lateralization at the peak frequency will then be tested using a t-test on power at ipsilateral vs. contralateral peak channels. Importantly, peak channels and frequencies are only selected on the basis of overall power, and not on the basis of showing the strongest lateralization.\n"}, "outliers": {"extra": [], "value": "All participants will have normal or corrected-to-normal vision, be right-handed, with no reported history of neurological or psychiatric disorders. Exclusion criteria are more than 10 interpolated electrodes or more than 40% rejected trials. Further, a minimum of 30 trials per cell is mandatory for inclusion into the study. Participants with a hit rate below 3 SDs of the mean will be excluded as well."}, "dependent": {"extra": [], "value": "ERP differences between fearful and neutral faces in six different conditions: Spatial attention to the center, left hemifield, right hemifield, having a continuous perceptual task with high or low difficulty. Specifcally, we examine the P1, N170, and the EPN component.  We will further extract and compare alpha power (see below secondary analyses)."}, "conditions": {"extra": [], "value": "Procedure. Participants will be instructed to avoid eye-movements and blinks during the stimulus presentation. While being prepared for the EEG, they will respond to a demographic questionnaire, as well as the BDI-II and STAI Trait questionnaire (Spielberger et al. 1999; Hautzinger et al. 2009), and a short version of the NEO-FFI (K\u00f6rner et al. 2008), which, however, will not be relevant for the current study. Counterbalanced, participants start with one of the six conditions (spatial attention: central attention block, attention 12\u00b0 to the left, or attention 12\u00b0 to the right field; perceptual difficulty: high or low). A fixation cross is presented constantly in the display centre. In all blocks, random dot kinematograms (RDK; e.g. Kelly and O\u2019Connell 2013) will be presented simultaneously within three circular apertures (radius = 3\u00b0) at the center, at the left, and at the right location. Each RDK contains 80 dots with a (radius = 0.04\u00b0). However, participants will be informed in advance which feature to attend to (high difficulty: coherent movement, low difficulty: colour change), and which position they should attend to. They will be informed that only at this location targets will ever appear, and will be informed ongoing about misses, as well hits and false alarms. For each of the six conditions, two blocks will be performed. Difficulty and task position alternate between the blocks. The order of alternation will be counterbalanced across subjects. In each block, 32 fearful and 32 neutral faces will be presented for 100 ms, followed by an inter-stimulus interval with a randomized duration between 2000 and 2200 ms. Thus for each of the six conditions 64 fearful and 64 neutral faces will be presented, leading to a total of 768 trials. All faces followed by a response or a target will be excluded from ERP analyses. The RDKs are continuously present throughout a block. Within each block, a pseudo-randomized number between 2 and 6 targets appear and pseudo-randomized points in time, defined by either all dots at the present target location turning blue, or by an onset of coherent motion. The latter refers to a proportion of dots moving exactly horizontally (direction chosen randomly for each target onset). The exact proportion (i.e. the coherence level) will be determined in pilot experiments and will be allowed to differ between centrally and peripherally presented RDKs to match the difficulty of central and peripheral tasks. To ensure that participants fixate the center, gaze position will be examined online with an eye tracker (EyeLink 1000, SR Research Ltd., Mississauga, Canada), stopping the presentation instantly whenever the centre is not fixated."}, "hypothesis": {"extra": [], "value": "Facial emotional expressions constitute a significant part of communication as they transfer non-verbal signals to others (Schyns et al. 2007). Especially for expressions signalling threat or danger, distinct early and late event-related potentials (ERPs) modulations are reported (M\u00fchlberger et al., 2009; Schindler, Bruchmann, Bublatzky, &amp; Straube, 2019; Schupp, \u00d6hman, et al., 2004; Yoon, Shim, Kim, &amp; Lee, 2016). However, until today there is an ongoing debate on the temporal and attentional constraints on extracting fearful expression information and it has not been clarified yet how task demands and spatial attention might alter such ERP modulations (Adolphs 2008; Pessoa and Adolphs 2010; Tamietto and de Gelder 2010; Pourtois et al. 2013). A recent review suggests that attentional manipulations differently modulate early and late ERPs (Schindler and Bublatzky in review). Here, we will examine ERPs to centrally presented task-irrelevant fearful and neutral faces, while participants have to attend either to the left, to the center, or to the right hemifield, responding to a sustained attention task inducing high or low perceptual difficulty.\nHypotheses: We will test how perceptual load and attended spatial position affect interactively or in parallel early (P1, N170), and mid-latency (EPN), emotion differentiation between fearful and neutral expressions. To this end, a continuous perceptual task of high or low difficulty will be presented either centrally, or in the left, or right hemifield. Centrally, task-irrelevant fearful and neutral faces are presented. Participants will have to fixate the center, which will be monitored by an eyetracker, stopping recording whenever the fixation falls outside of the center. Participants will have to monitor continuously random moving dots, and respond to short periods of coherent movements (high difficulty), or to color changes (low difficulty). To validate that participants have indeed shifted spatial attention as instructed at the moment of face presentation, we will examine lateralized alpha power in an interval directly preceding face presentation (Worden et al. 2000; Sauseng et al. 2005; Kelly et al. 2006; Horschig et al. 2014). Further, we will compare alpha activity for blocks with high versus low perceptual difficulty.\nRegarding ERPs, we expect interactions of emotion and attended spatial position, for the P1 and N170, where emotion effects (i.e. larger amplitudes for fearful faces) should be strongest when participants are asked to attend the central position. Based on our previous studies, we do not expect perceptual load by emotion interactions for the P1 and N170. Further, as emotional differences during the EPN range seem to be more vulnerable to load, we might find a three-way interaction of attended spatial position, perceptual load, and emotion. Here, an emotional differentiation for the EPN should only be observed when participants attend to the central position under low perceptual load. "}, "study_type": {"extra": [], "value": "Experiment"}, "study_type_other": {"extra": [], "value": "For further information, please refer to the word file attached to this registration."}}, "registration_responses": {"data": "No, no data have been collected for this study yet.", "name": "Effects of load and spatial attention on neuronal responses to fearful and neutral faces", "other": "To ensure that participants shift their attention to the instructed left and right locations, power spectra will be computed by means of a Fast Fourier Transform of single-trial raw data in a one-second long pre-stimulus time window and subsequent averaging across trials (see above). Further, we will compare alpha activity for blocks with high versus low perceptual difficulty.", "sample": "Participants. In total, the data sampling plan is designed to examine 40 participants, as power calculations using G*Power 3.1.7 (Faul et al. 2009) show that sampling 40 participants leads to a power of &gt; 90% to detect medium effects sizes (f = 0.25), for a within two by three repeated-measure ANOVA designs. ", "analyses": "Data analyses. Behavioural data will be analysed with JASP (www.jasp.org/), using a two (Load: high vs low) by three level (Attention location: left, central, right) repeated measure ANOVA, to investigate main effects of task load and attentional location as secondary analyses. \nEEG scalp-data will be statistically analyzed with EMEGS. Here, for each ERP component, a two (Emotion: fearful, neutral) by two (Load: high vs low) by three (Attention location: left, central, right) repeated measure ANOVA is used investigate main effects of attentional location and emotional expression and, as well as their interaction. Partial eta-squared (partial \u03b72) will be used to describe effect sizes, where \u03b7P2 = 0.02 describes a small, \u03b7P2 = 0.13 a medium and \u03b7P2 = 0.26 a large effect (Cohen, 1988). For the EPN and LPP, typically scored as differences between emotional and neutral stimuli, all task conditions will be collapsed to identify emotion effects. Time windows will be approximately segmented from 80 to 100 ms for the P1, 130 to 170 ms for the N170, from 200 to 350 ms to investigate EPN effects, and from 400 to 600 ms to investigate LPP effects. Based on previous pre-registered studies using faces across tasks (Bruchmann et al. in preparation; Schindler, Bruchmann, Bublatzky, et al. 2019; Schindler, Bruchmann, Gathmann, et al. 2019), we expect to score for the P1, N170, and EPN time window over two symmetrical occipital clusters (left P9, P7, PO7; right P10, P8, PO8). However, please note, that this can be due to task effects shifted either to more temporal or occipital regions, which will be examined by the above-mentioned collapsed localizer. \nTo ensure that participants shift their attention to the instructed location presentation, power spectra will be computed by means of a Fast Fourier Transform of single-trial raw data in a one-second long pre-stimulus time window and subsequent averaging across trials. To account for interindividual variability in topography and frequency (Haegens et al. 2014) of the alpha rhythm, we will identify individual peak channels and peak frequencies. Peak frequencies will be defined as the frequency with the strongest power within a 5 \u2013 15 Hz range. Peak channels will be defined as the set of 10 posterior channels showing the strongest power at the peak frequency. Alpha lateralization at the peak frequency will then be tested using a t-test on power at ipsilateral vs. contralateral peak channels. Importantly, peak channels and frequencies are only selected on the basis of overall power, and not on the basis of showing the strongest lateralization.\n", "outliers": "All participants will have normal or corrected-to-normal vision, be right-handed, with no reported history of neurological or psychiatric disorders. Exclusion criteria are more than 10 interpolated electrodes or more than 40% rejected trials. Further, a minimum of 30 trials per cell is mandatory for inclusion into the study. Participants with a hit rate below 3 SDs of the mean will be excluded as well.", "dependent": "ERP differences between fearful and neutral faces in six different conditions: Spatial attention to the center, left hemifield, right hemifield, having a continuous perceptual task with high or low difficulty. Specifcally, we examine the P1, N170, and the EPN component.  We will further extract and compare alpha power (see below secondary analyses).", "conditions": "Procedure. Participants will be instructed to avoid eye-movements and blinks during the stimulus presentation. While being prepared for the EEG, they will respond to a demographic questionnaire, as well as the BDI-II and STAI Trait questionnaire (Spielberger et al. 1999; Hautzinger et al. 2009), and a short version of the NEO-FFI (K\u00f6rner et al. 2008), which, however, will not be relevant for the current study. Counterbalanced, participants start with one of the six conditions (spatial attention: central attention block, attention 12\u00b0 to the left, or attention 12\u00b0 to the right field; perceptual difficulty: high or low). A fixation cross is presented constantly in the display centre. In all blocks, random dot kinematograms (RDK; e.g. Kelly and O\u2019Connell 2013) will be presented simultaneously within three circular apertures (radius = 3\u00b0) at the center, at the left, and at the right location. Each RDK contains 80 dots with a (radius = 0.04\u00b0). However, participants will be informed in advance which feature to attend to (high difficulty: coherent movement, low difficulty: colour change), and which position they should attend to. They will be informed that only at this location targets will ever appear, and will be informed ongoing about misses, as well hits and false alarms. For each of the six conditions, two blocks will be performed. Difficulty and task position alternate between the blocks. The order of alternation will be counterbalanced across subjects. In each block, 32 fearful and 32 neutral faces will be presented for 100 ms, followed by an inter-stimulus interval with a randomized duration between 2000 and 2200 ms. Thus for each of the six conditions 64 fearful and 64 neutral faces will be presented, leading to a total of 768 trials. All faces followed by a response or a target will be excluded from ERP analyses. The RDKs are continuously present throughout a block. Within each block, a pseudo-randomized number between 2 and 6 targets appear and pseudo-randomized points in time, defined by either all dots at the present target location turning blue, or by an onset of coherent motion. The latter refers to a proportion of dots moving exactly horizontally (direction chosen randomly for each target onset). The exact proportion (i.e. the coherence level) will be determined in pilot experiments and will be allowed to differ between centrally and peripherally presented RDKs to match the difficulty of central and peripheral tasks. To ensure that participants fixate the center, gaze position will be examined online with an eye tracker (EyeLink 1000, SR Research Ltd., Mississauga, Canada), stopping the presentation instantly whenever the centre is not fixated.", "hypothesis": "Facial emotional expressions constitute a significant part of communication as they transfer non-verbal signals to others (Schyns et al. 2007). Especially for expressions signalling threat or danger, distinct early and late event-related potentials (ERPs) modulations are reported (M\u00fchlberger et al., 2009; Schindler, Bruchmann, Bublatzky, &amp; Straube, 2019; Schupp, \u00d6hman, et al., 2004; Yoon, Shim, Kim, &amp; Lee, 2016). However, until today there is an ongoing debate on the temporal and attentional constraints on extracting fearful expression information and it has not been clarified yet how task demands and spatial attention might alter such ERP modulations (Adolphs 2008; Pessoa and Adolphs 2010; Tamietto and de Gelder 2010; Pourtois et al. 2013). A recent review suggests that attentional manipulations differently modulate early and late ERPs (Schindler and Bublatzky in review). Here, we will examine ERPs to centrally presented task-irrelevant fearful and neutral faces, while participants have to attend either to the left, to the center, or to the right hemifield, responding to a sustained attention task inducing high or low perceptual difficulty.\nHypotheses: We will test how perceptual load and attended spatial position affect interactively or in parallel early (P1, N170), and mid-latency (EPN), emotion differentiation between fearful and neutral expressions. To this end, a continuous perceptual task of high or low difficulty will be presented either centrally, or in the left, or right hemifield. Centrally, task-irrelevant fearful and neutral faces are presented. Participants will have to fixate the center, which will be monitored by an eyetracker, stopping recording whenever the fixation falls outside of the center. Participants will have to monitor continuously random moving dots, and respond to short periods of coherent movements (high difficulty), or to color changes (low difficulty). To validate that participants have indeed shifted spatial attention as instructed at the moment of face presentation, we will examine lateralized alpha power in an interval directly preceding face presentation (Worden et al. 2000; Sauseng et al. 2005; Kelly et al. 2006; Horschig et al. 2014). Further, we will compare alpha activity for blocks with high versus low perceptual difficulty.\nRegarding ERPs, we expect interactions of emotion and attended spatial position, for the P1 and N170, where emotion effects (i.e. larger amplitudes for fearful faces) should be strongest when participants are asked to attend the central position. Based on our previous studies, we do not expect perceptual load by emotion interactions for the P1 and N170. Further, as emotional differences during the EPN range seem to be more vulnerable to load, we might find a three-way interaction of attended spatial position, perceptual load, and emotion. Here, an emotional differentiation for the EPN should only be observed when participants attend to the central position under low perceptual load. ", "study_type": "Experiment", "study_type_other": "For further information, please refer to the word file attached to this registration."}, "subjects": []}, "relationships": {"children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/comments/?format=json&filter%5Btarget%5D=utb2e", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/utb2e/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/de-1/?format=json", "meta": {}}}, "data": {"id": "de-1", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/?format=json", "meta": {}}}, "data": {"id": "utb2e", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/utb2e/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/utb2e/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/utb2e/citation/?format=json", "meta": {}}}, "data": {"id": "utb2e", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/auxwp/?format=json", "meta": {}}}, "data": {"id": "auxwp", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/entb8/?format=json", "meta": {}}}, "data": {"id": "entb8", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5d2d2268d28338002c2432d2/?format=json", "meta": {}}}, "data": {"id": "5d2d2268d28338002c2432d2", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/utb2e/", "self": "https://api.osf.io/v2/registrations/utb2e/"}}, {"id": "r69jd", "type": "registrations", "attributes": {"title": "The pain-attenuating effects of social support", "description": "", "category": "project", "custom_citation": "", "date_created": "2020-01-28T00:32:05.212455", "date_modified": "2020-01-27T23:55:31.009540", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": null, "analytics_key": "3fec5c3feb8b3910f02dc641abfae70a6e0a54dc534515841cb9a2badce69786f651309dc6e940ef87bb89e5a775ce6d102733015e6be0c89aadb750f17adc2e602c377803e0df171072aefde438dfa8c341298fba6fe089db795ddfda9c41fa8fe0257231a7d0f1a1139760ddbfb4ea28458824f59d7e00b150b52e758191280a1b67bf184021150403819e36f16ec5", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-28T00:32:05.194954", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "Pre-Registration in Social Psychology (van 't Veer & Giner-Sorolla, 2016): Pre-Registration", "registered_meta": {"looked": {"extra": [], "value": "No"}, "datacompletion": {"extra": [], "value": "No, data collection has not begun"}, "additionalComments": {"extra": [], "value": ""}, "dataCollectionDates": {"extra": [], "value": "Start date: 15 February 2020\nEnd date: 15 April 2020"}, "description-methods": {"value": {"design": {"value": {"question2a": {"extra": [], "value": "The independent variable, social support, is between-participant, and has three levels:\n- 'Partner present' (the participant will hold her partner's hand during the cold pressor task)\n- 'Mental representation' (the participant will look at a photo of her partner during the cold pressor task)\n- 'Control' (no social support: the participant will complete the cold pressor task alone)"}, "question2b": {"extra": [], "value": "The dependent variable is the level of pain reported by participants, on a scale of 1 to 7"}, "question3b": {"extra": [], "value": "There are no covariates or moderators."}}}, "procedure": {"value": {"question10b": {"extra": [], "value": "Participants will be invited to come to the lab with their romantic partner. Upon each couple's arrival, an experimenter will send the participant to one room, and her partner to another. \n\nParticipants will be randomly assigned to one of the three social support conditions (partner present, mental representation, and control), after which they will undertake the cold pressor task (CPT). Before starting the CPT, participants will fill out a brief socio-demographic questionnaire.\n\nExperimental conditions:\n\nPartner present:\nWhilst the participant waits for the CPT to be prepared by experimenter 1, her partner will be briefed on his role in the procedure by experimenter 2: he will be informed that his partner will be asked to maintain her dominant hand in a tank of cold water, and that he will hold her other hand throughout the task. He will then be escorted to the CPT room for the task.\n\nMental representation:\nWhile the participant waits for the CPT to be prepared by experimenter 1, her partner will be photographed by experimenter 2, who will then load the partner's photo on a computer screen in the CPT room. During the CPT, the participant's partner's photo will be displayed on the computer screen for her to see.\n\nControl:\nThe participant will undergo the CPT alone, while her partner remains in the other room.\n\n\nCPT procedure (cf. Roberts, Klatzkin, &amp; Mechlin, 2015):\nParticipants will submerge their dominant hand in a tank of 4 degrees celsius water, and keep it there for as long as is tolerable, or for a maximum of 3 minutes. Participants will be asked to indicate the level of pain felt on a scale of 1 to 7 (DV).\n\nAt the end of the experiment, participants will be fully debriefed and asked whether they consent to their data being analysed.\n"}}}, "planned-sample": {"value": {"question4b": {"extra": [], "value": "159 female participants currently in a romantic relationship will be recruited via posters displayed in various locations of the university campus. They will be invited, along with their partner, to participate in a study investigating how the menstrual cycle influences perception of pain (cf. cover story by Roberts, Klatzkin, &amp; Mechlin, 2015) in exchange for 40 CHF per couple."}, "question5b": {"extra": [], "value": "Data will be collected at the University of Geneva, in the social psychology laboratory, by two experimenters."}, "question6b": {"extra": [], "value": "The sample size was determined by carrying out power analysis using G*Power, with the following parameters:\n- analysis: ANOVA\n- effect size: f = 0.25\n- alpha = 0.05\n- power = 0.8\n- number of groups = 3"}, "question7b": {"extra": [], "value": "Data collection will stop once 159 subjects have participated and given their consent for their data to be analysed."}, "question6b-upload": {"extra": [{"data": {"name": "Screenshot GPower.png"}, "nodeId": "dhj67", "sha256": "9ac325f209c7fac09955d591d6aa2f2e942cab44c2b03d35d0500a70ad841508", "viewUrl": "/project/r69jd/files/osfstorage/5e2f8108e71ef8004918878f/", "selectedFileName": "Screenshot GPower.png"}], "value": ""}}}, "exclusion-criteria": {"value": {"question8b": {"extra": [], "value": "There are no exclusion criteria."}}}}}, "recommended-methods": {"value": {"procedure": {"value": {"question9b": {"extra": [], "value": ""}, "question9b-file": {"extra": [], "value": ""}}}}}, "recommended-analysis": {"value": {"specify": {"value": {"question6c": {"extra": [], "value": ""}, "question7c": {"extra": [], "value": ""}, "question8c": {"extra": [], "value": ""}, "question9c": {"extra": [], "value": ""}, "question10c": {"extra": [], "value": ""}, "question11c": {"extra": [], "value": ""}}}}}, "description-hypothesis": {"value": {"question1a": {"extra": [], "value": "We expect that participants in the 'mental representation' condition and those in the 'partner  present' condition will report feeling lower levels of pain than participants in the control condition. We expect that reported pain levels in the 'mental representation' and 'partner present' conditions will not differ significantly."}, "question2a": {"extra": [], "value": "We do not expect any interaction effects."}, "question3a": {"extra": [], "value": "No manipulation check is included."}}}, "recommended-hypothesis": {"value": {"question4a": {"extra": [], "value": ""}, "question5a": {"extra": [], "value": ""}, "question6a": {"extra": [], "value": ""}}}, "confirmatory-analyses-first": {"value": {"first": {"value": {"question1c": {"extra": [], "value": "The dependent variable will be measured with the following question:  \"How much pain did you feel, on a scale of 1 (no pain at all) to 7 (severe pain)?\""}, "question2c": {"extra": [], "value": "We will conduct analysis of variance."}, "question3c": {"extra": [], "value": "IV: social support\nDV: level of pain\n"}, "question4c": {"extra": [], "value": "There are no covariates."}, "question5c": {"extra": [], "value": "We expect that reported pain levels will be significantly lower in the 'partner present' and in the 'mental representation' conditions than in the control condition."}}}}}, "confirmatory-analyses-third": {"value": {"third": {"value": {"question1c": {"extra": [], "value": ""}, "question2c": {"extra": [], "value": ""}, "question3c": {"extra": [], "value": ""}, "question4c": {"extra": [], "value": ""}, "question5c": {"extra": [], "value": ""}}}}}, "confirmatory-analyses-fourth": {"value": {"fourth": {"value": {"question1c": {"extra": [], "value": ""}, "question2c": {"extra": [], "value": ""}, "question3c": {"extra": [], "value": ""}, "question4c": {"extra": [], "value": ""}, "question5c": {"extra": [], "value": ""}}}}}, "confirmatory-analyses-second": {"value": {"second": {"value": {"question1c": {"extra": [], "value": ""}, "question2c": {"extra": [], "value": ""}, "question3c": {"extra": [], "value": ""}, "question4c": {"extra": [], "value": ""}, "question5c": {"extra": [], "value": ""}}}}}, "confirmatory-analyses-further": {"value": {"further": {"value": {"question1c": {"extra": [], "value": ""}, "question2c": {"extra": [], "value": ""}, "question3c": {"extra": [], "value": ""}, "question4c": {"extra": [], "value": ""}, "question5c": {"extra": [], "value": ""}}}}}}, "registration_responses": {"looked": "No", "datacompletion": "No, data collection has not begun", "additionalComments": "", "dataCollectionDates": "Start date: 15 February 2020\nEnd date: 15 April 2020", "description-hypothesis.question1a": "We expect that participants in the 'mental representation' condition and those in the 'partner  present' condition will report feeling lower levels of pain than participants in the control condition. We expect that reported pain levels in the 'mental representation' and 'partner present' conditions will not differ significantly.", "description-hypothesis.question2a": "We do not expect any interaction effects.", "description-hypothesis.question3a": "No manipulation check is included.", "recommended-hypothesis.question4a": [], "recommended-hypothesis.question5a": "", "recommended-hypothesis.question6a": "", "description-methods.design.question2a": "The independent variable, social support, is between-participant, and has three levels:\n- 'Partner present' (the participant will hold her partner's hand during the cold pressor task)\n- 'Mental representation' (the participant will look at a photo of her partner during the cold pressor task)\n- 'Control' (no social support: the participant will complete the cold pressor task alone)", "description-methods.design.question2b": "The dependent variable is the level of pain reported by participants, on a scale of 1 to 7", "description-methods.design.question3b": "There are no covariates or moderators.", "recommended-analysis.specify.question6c": "", "recommended-analysis.specify.question7c": "", "recommended-analysis.specify.question8c": "", "recommended-analysis.specify.question9c": "", "recommended-analysis.specify.question10c": "", "recommended-analysis.specify.question11c": [], "recommended-methods.procedure.question9b": "", "description-methods.procedure.question10b": "Participants will be invited to come to the lab with their romantic partner. Upon each couple's arrival, an experimenter will send the participant to one room, and her partner to another. \n\nParticipants will be randomly assigned to one of the three social support conditions (partner present, mental representation, and control), after which they will undertake the cold pressor task (CPT). Before starting the CPT, participants will fill out a brief socio-demographic questionnaire.\n\nExperimental conditions:\n\nPartner present:\nWhilst the participant waits for the CPT to be prepared by experimenter 1, her partner will be briefed on his role in the procedure by experimenter 2: he will be informed that his partner will be asked to maintain her dominant hand in a tank of cold water, and that he will hold her other hand throughout the task. He will then be escorted to the CPT room for the task.\n\nMental representation:\nWhile the participant waits for the CPT to be prepared by experimenter 1, her partner will be photographed by experimenter 2, who will then load the partner's photo on a computer screen in the CPT room. During the CPT, the participant's partner's photo will be displayed on the computer screen for her to see.\n\nControl:\nThe participant will undergo the CPT alone, while her partner remains in the other room.\n\n\nCPT procedure (cf. Roberts, Klatzkin, &amp; Mechlin, 2015):\nParticipants will submerge their dominant hand in a tank of 4 degrees celsius water, and keep it there for as long as is tolerable, or for a maximum of 3 minutes. Participants will be asked to indicate the level of pain felt on a scale of 1 to 7 (DV).\n\nAt the end of the experiment, participants will be fully debriefed and asked whether they consent to their data being analysed.\n", "confirmatory-analyses-first.first.question1c": "The dependent variable will be measured with the following question:  \"How much pain did you feel, on a scale of 1 (no pain at all) to 7 (severe pain)?\"", "confirmatory-analyses-first.first.question2c": "We will conduct analysis of variance.", "confirmatory-analyses-first.first.question3c": "IV: social support\nDV: level of pain\n", "confirmatory-analyses-first.first.question4c": "There are no covariates.", "confirmatory-analyses-first.first.question5c": "We expect that reported pain levels will be significantly lower in the 'partner present' and in the 'mental representation' conditions than in the control condition.", "confirmatory-analyses-third.third.question1c": "", "confirmatory-analyses-third.third.question2c": "", "confirmatory-analyses-third.third.question3c": "", "confirmatory-analyses-third.third.question4c": "", "confirmatory-analyses-third.third.question5c": "", "description-methods.planned-sample.question4b": "159 female participants currently in a romantic relationship will be recruited via posters displayed in various locations of the university campus. They will be invited, along with their partner, to participate in a study investigating how the menstrual cycle influences perception of pain (cf. cover story by Roberts, Klatzkin, &amp; Mechlin, 2015) in exchange for 40 CHF per couple.", "description-methods.planned-sample.question5b": "Data will be collected at the University of Geneva, in the social psychology laboratory, by two experimenters.", "description-methods.planned-sample.question6b": "The sample size was determined by carrying out power analysis using G*Power, with the following parameters:\n- analysis: ANOVA\n- effect size: f = 0.25\n- alpha = 0.05\n- power = 0.8\n- number of groups = 3", "description-methods.planned-sample.question7b": "Data collection will stop once 159 subjects have participated and given their consent for their data to be analysed.", "recommended-methods.procedure.question9b-file": [], "confirmatory-analyses-fourth.fourth.question1c": "", "confirmatory-analyses-fourth.fourth.question2c": "", "confirmatory-analyses-fourth.fourth.question3c": "", "confirmatory-analyses-fourth.fourth.question4c": "", "confirmatory-analyses-fourth.fourth.question5c": "", "confirmatory-analyses-second.second.question1c": "", "confirmatory-analyses-second.second.question2c": "", "confirmatory-analyses-second.second.question3c": "", "confirmatory-analyses-second.second.question4c": "", "confirmatory-analyses-second.second.question5c": "", "confirmatory-analyses-further.further.question1c": "", "confirmatory-analyses-further.further.question2c": "", "confirmatory-analyses-further.further.question3c": "", "confirmatory-analyses-further.further.question4c": "", "confirmatory-analyses-further.further.question5c": "", "description-methods.exclusion-criteria.question8b": "There are no exclusion criteria.", "description-methods.planned-sample.question6b-upload": [{"file_id": "5e2f8108e71ef8004918878f", "file_name": "Screenshot GPower.png", "file_urls": {"html": "https://osf.io/project/r69jd/files/osfstorage/5e2f8108e71ef8004918878f", "download": "https://osf.io/download/5e2f8108e71ef8004918878f"}, "file_hashes": {"sha256": "9ac325f209c7fac09955d591d6aa2f2e942cab44c2b03d35d0500a70ad841508"}}]}, "subjects": []}, "relationships": {"children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/comments/?format=json&filter%5Btarget%5D=r69jd", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/r69jd/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/de-1/?format=json", "meta": {}}}, "data": {"id": "de-1", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/?format=json", "meta": {}}}, "data": {"id": "r69jd", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/r69jd/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/r69jd/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/r69jd/citation/?format=json", "meta": {}}}, "data": {"id": "r69jd", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/e5fr9/?format=json", "meta": {}}}, "data": {"id": "e5fr9", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/dhj67/?format=json", "meta": {}}}, "data": {"id": "dhj67", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5730e99a9ad5a102c5745a8a/?format=json", "meta": {}}}, "data": {"id": "5730e99a9ad5a102c5745a8a", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/r69jd/", "self": "https://api.osf.io/v2/registrations/r69jd/"}}, {"id": "6yhtg", "type": "registrations", "attributes": {"title": "Religious Identity and Psychological Distress After Facing Tragedy", "description": "", "category": "project", "custom_citation": "", "date_created": "2020-01-27T21:17:59.431132", "date_modified": "2019-09-30T14:54:05.815155", "registration": true, "preprint": false, "fork": false, "collection": false, "tags": [], "access_requests_enabled": false, "node_license": null, "analytics_key": "32712d3a7fc7858ed213956871d68e2b6fb1d460d594658079ba04e7f64d235659bda9fdb8474b07de8f17f33c6465242f994a4423bb3f872ee195619ca097659edbc0ce5176cac3b31a9fcf64b02d2119f3c42c56dd13a0bc049aaca52f5c0d8fc9049bbe86fac024ba2d0201392f2ce0c890b1effc5fe41c9781141c75955a8bbf907b4c01e19e63ebdbe6af121d5c", "current_user_can_comment": false, "current_user_permissions": ["read"], "current_user_is_contributor": false, "current_user_is_contributor_or_group_member": false, "wiki_enabled": true, "public": true, "article_doi": null, "pending_embargo_approval": false, "pending_embargo_termination_approval": false, "embargoed": false, "pending_registration_approval": false, "archiving": false, "pending_withdrawal": false, "withdrawn": false, "date_registered": "2020-01-27T21:17:59.414869", "date_withdrawn": null, "embargo_end_date": null, "withdrawal_justification": null, "registration_supplement": "Preregistration Template from AsPredicted.org", "registered_meta": {"data": {"extra": [], "value": "It's complicated. We have already collected some data but explain in Question 8 why readers may consider this a valid pre-registration nevertheless."}, "name": {"extra": [], "value": "(Non)Religiosity, Distress, Coping, and Posttraumatic Growth Following a Natural Disaster"}, "other": {"extra": [], "value": ""}, "sample": {"extra": [], "value": "We will recruit a minimum of 100 participants to allow for adequate power for testing relationships between variables; our sample size will be limited because it is restricted to participants who experienced the natural disaster. The PROCESS macro will use bootstrapping to estimate confidence intervals. "}, "analyses": {"extra": [], "value": "We will use the SPSS Process Macro to run alternative models testing for Intensity of event as a predictor (X) of Distress and Post-traumatic growth (Y), through coping style (mediator), with Nonreligiosity (NR) and Nonspirituality as a moderating variables of these effects. "}, "outliers": {"extra": [], "value": "We do not anticipate outliers based on the measures employed. We will, however, only use data from participants who complete the full set of measures for testing relationships between variables. "}, "dependent": {"extra": [], "value": "Key variables are religiosity/spirituality (assessed using the Nonreligious-Nonspiritual Scale; NRNSS), Brief Cope Inventory, Posttraumatic Growth Inventory, and Depression Scale. "}, "conditions": {"extra": [], "value": "Participants will not be assigned to conditions. They will be selected based on their experience with a local natural disaster event and differences on the NRNSS scale will be treated as moderating conditions. "}, "hypothesis": {"extra": [], "value": "We are collecting data from adult participants who experienced a natural disaster (tornado) to test whether (non)religiosity and (non)spirituality moderate relationships between distress and post-traumatic growth. "}, "study_type": {"extra": [], "value": "Survey"}, "study_type_other": {"extra": [], "value": "Data is already in collection due to the time-sensitive nature of measuring participants' experiences related to the natural disaster. The data have not been assessed and no analyses were conducted prior to pre-registration of the project. "}}, "registration_responses": {"data": "It's complicated. We have already collected some data but explain in Question 8 why readers may consider this a valid pre-registration nevertheless.", "name": "(Non)Religiosity, Distress, Coping, and Posttraumatic Growth Following a Natural Disaster", "sample": "We will recruit a minimum of 100 participants to allow for adequate power for testing relationships between variables; our sample size will be limited because it is restricted to participants who experienced the natural disaster. The PROCESS macro will use bootstrapping to estimate confidence intervals. ", "analyses": "We will use the SPSS Process Macro to run alternative models testing for Intensity of event as a predictor (X) of Distress and Post-traumatic growth (Y), through coping style (mediator), with Nonreligiosity (NR) and Nonspirituality as a moderating variables of these effects. ", "outliers": "We do not anticipate outliers based on the measures employed. We will, however, only use data from participants who complete the full set of measures for testing relationships between variables. ", "dependent": "Key variables are religiosity/spirituality (assessed using the Nonreligious-Nonspiritual Scale; NRNSS), Brief Cope Inventory, Posttraumatic Growth Inventory, and Depression Scale. ", "conditions": "Participants will not be assigned to conditions. They will be selected based on their experience with a local natural disaster event and differences on the NRNSS scale will be treated as moderating conditions. ", "hypothesis": "We are collecting data from adult participants who experienced a natural disaster (tornado) to test whether (non)religiosity and (non)spirituality moderate relationships between distress and post-traumatic growth. ", "study_type": "Survey", "study_type_other": "Data is already in collection due to the time-sensitive nature of measuring participants' experiences related to the natural disaster. The data have not been assessed and no analyses were conducted prior to pre-registration of the project. "}, "subjects": []}, "relationships": {"children": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/children/?format=json", "meta": {}}}}, "comments": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/comments/?format=json&filter%5Btarget%5D=6yhtg", "meta": {}}}}, "contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/contributors/?format=json", "meta": {}}}}, "bibliographic_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/bibliographic_contributors/?format=json", "meta": {}}}}, "implicit_contributors": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/implicit_contributors/?format=json", "meta": {}}}}, "files": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/files/?format=json", "meta": {}}}}, "wikis": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/wikis/?format=json", "meta": {}}}}, "forks": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/forks/?format=json", "meta": {}}}}, "node_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/node_links/?format=json", "meta": {}}}}, "linked_by_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/linked_by_nodes/?format=json", "meta": {}}}}, "linked_by_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/linked_by_registrations/?format=json", "meta": {}}}}, "identifiers": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/identifiers/?format=json", "meta": {}}}}, "affiliated_institutions": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/institutions/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/6yhtg/relationships/institutions/?format=json", "meta": {}}}}, "region": {"links": {"related": {"href": "https://api.osf.io/v2/regions/us/?format=json", "meta": {}}}, "data": {"id": "us", "type": "regions"}}, "root": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/?format=json", "meta": {}}}, "data": {"id": "6yhtg", "type": "registrations"}}, "logs": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/logs/?format=json", "meta": {}}}}, "linked_nodes": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/linked_nodes/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/6yhtg/relationships/linked_nodes/?format=json", "meta": {}}}}, "linked_registrations": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/linked_registrations/?format=json", "meta": {}}, "self": {"href": "https://api.osf.io/v2/registrations/6yhtg/relationships/linked_registrations/?format=json", "meta": {}}}}, "view_only_links": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/view_only_links/?format=json", "meta": {}}}}, "citation": {"links": {"related": {"href": "https://api.osf.io/v2/registrations/6yhtg/citation/?format=json", "meta": {}}}, "data": {"id": "6yhtg", "type": "registrations"}}, "registered_by": {"links": {"related": {"href": "https://api.osf.io/v2/users/sqr5e/?format=json", "meta": {}}}, "data": {"id": "sqr5e", "type": "users"}}, "registered_from": {"links": {"related": {"href": "https://api.osf.io/v2/nodes/8a234/?format=json", "meta": {}}}, "data": {"id": "8a234", "type": "nodes"}}, "registration_schema": {"links": {"related": {"href": "https://api.osf.io/v2/schemas/registrations/5d2d2268d28338002c2432d2/?format=json", "meta": {}}}, "data": {"id": "5d2d2268d28338002c2432d2", "type": "registration-schemas"}}, "provider": {"links": {"related": {"href": "https://api.osf.io/v2/providers/registrations/osf/?format=json", "meta": {}}}}}, "links": {"html": "https://osf.io/6yhtg/", "self": "https://api.osf.io/v2/registrations/6yhtg/"}}], "links": {"first": "https://api.osf.io/v2/registrations/?filter%5Bdate_created%5D%5Bgt%5D=2019-12-31&format=json", "last": "https://api.osf.io/v2/registrations/?filter%5Bdate_created%5D%5Bgt%5D=2019-12-31&format=json&page=547", "prev": "https://api.osf.io/v2/registrations/?filter%5Bdate_created%5D%5Bgt%5D=2019-12-31&format=json&page=480", "next": "https://api.osf.io/v2/registrations/?filter%5Bdate_created%5D%5Bgt%5D=2019-12-31&format=json&page=482", "meta": {"total": 5469, "per_page": 10}}}